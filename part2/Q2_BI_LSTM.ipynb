{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import string\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchviz import make_dot\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(word2vec.index_to_key)\n",
    "vocab_size = len(word2vec.index_to_key)\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('dataset part 2/train.csv')\n",
    "test_df = pd.read_csv(\"dataset part 2/test.csv\")\n",
    "train_df.drop(columns=['label-fine'], inplace=True)\n",
    "test_df.drop(columns=['label-fine'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>1</td>\n",
       "      <td>What 's the shape of a camel 's spine ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>1</td>\n",
       "      <td>What type of currency is used in China ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>4</td>\n",
       "      <td>What is the temperature today ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>4</td>\n",
       "      <td>What is the temperature for cooking ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451</th>\n",
       "      <td>1</td>\n",
       "      <td>What currency is used in Australia ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5452 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label-coarse                                               text\n",
       "0                0  How did serfdom develop in and then leave Russ...\n",
       "1                1   What films featured the character Popeye Doyle ?\n",
       "2                0  How can I find a list of celebrities ' real na...\n",
       "3                1  What fowl grabs the spotlight after the Chines...\n",
       "4                2                    What is the full form of .com ?\n",
       "...            ...                                                ...\n",
       "5447             1            What 's the shape of a camel 's spine ?\n",
       "5448             1           What type of currency is used in China ?\n",
       "5449             4                    What is the temperature today ?\n",
       "5450             4              What is the temperature for cooking ?\n",
       "5451             1               What currency is used in Australia ?\n",
       "\n",
       "[5452 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form a development set from a random subset (containing 500 examples) within the original training data. Remove these examples from original training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_df = train_df.sample(n=500, random_state=0) # randomly sample 500 rows from train df\n",
    "train_df = train_df.drop(development_df.index) # remove sampled rows from train df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>4</td>\n",
       "      <td>How many trees go into paper making in a year ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>1</td>\n",
       "      <td>What concerts are held in New York this week ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>5</td>\n",
       "      <td>Where did the sport of caber-tossing originate ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>3</td>\n",
       "      <td>What kind of people took part in Shays ' Rebel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2812</th>\n",
       "      <td>0</td>\n",
       "      <td>How is cologne made ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5335</th>\n",
       "      <td>5</td>\n",
       "      <td>Where was George Washington born ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>4</td>\n",
       "      <td>In what year did the Bounty mutiny happen ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4508</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the difference between khaki and chino ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>4</td>\n",
       "      <td>How many types of dogs ' tails are there - three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>0</td>\n",
       "      <td>What is a `` node '' in computer terms ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label-coarse                                               text\n",
       "2755             4    How many trees go into paper making in a year ?\n",
       "3326             1     What concerts are held in New York this week ?\n",
       "2204             5   Where did the sport of caber-tossing originate ?\n",
       "2888             3  What kind of people took part in Shays ' Rebel...\n",
       "2812             0                              How is cologne made ?\n",
       "...            ...                                                ...\n",
       "5335             5                 Where was George Washington born ?\n",
       "1275             4        In what year did the Bounty mutiny happen ?\n",
       "4508             0   What is the difference between khaki and chino ?\n",
       "402              4   How many types of dogs ' tails are there - three\n",
       "4386             0           What is a `` node '' in computer terms ?\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "development_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select 4 classes from 6 coarse labels, combine remaining 2 to form a single class 'OTHERS'. Adjust original data such that label for each sentence is updated accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected 2 labels for combining\n",
    "combined_1 = 4\n",
    "combined_2 = 5\n",
    "\n",
    "# train_df['label-coarse'] = train_df['label-coarse'].astype(object)\n",
    "# test_df['label-coarse'] = train_df['label-coarse'].astype(object)\n",
    "# development_df['label-coarse'] = train_df['label-coarse'].astype(object)\n",
    "\n",
    "# Assign label 5 as 4 as the OTHERS label\n",
    "train_df.loc[(train_df['label-coarse'] == combined_2), 'label-coarse'] = combined_1\n",
    "test_df.loc[(test_df['label-coarse'] == combined_2), 'label-coarse'] = combined_1\n",
    "development_df.loc[(development_df['label-coarse'] == combined_2), 'label-coarse'] = combined_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_and_remove_punctuation(text):\n",
    "    # Create a translation table to replace punctuation with empty strings\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # Remove punctuation and replace with ''\n",
    "    text_ = text.translate(translator)\n",
    "    tokens = word_tokenize(text_)\n",
    "    # filtered_tokens = [token for token in tokens if token in vocab]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].apply(lambda x: tokenize_and_remove_punctuation(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: tokenize_and_remove_punctuation(x))\n",
    "development_df['text'] = development_df['text'].apply(lambda x: tokenize_and_remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding of Data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pad_tokens(tokens, max_length, padding_value='<pad>'):\n",
    "#     if len(tokens) < max_length:\n",
    "#         tokens.extend([padding_value] * (max_length - len(tokens)))\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = max(train_df['text'].apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['text'] = train_df['text'].apply(pad_tokens, max_length=max_length)\n",
    "# test_df['text'] = test_df['text'].apply(pad_tokens, max_length=max_length)\n",
    "# development_df['text'] = development_df['text'].apply(pad_tokens, max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embedding matrix for embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = word2vec.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last element a zero array for out of vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_array = np.zeros((300,))\n",
    "embedding_matrix = np.vstack((embedding_matrix, zero_array)) # last element a zero array for OOV words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode text in dataframe to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_indices(tokens, model):\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            indices.append(model.get_index(token))\n",
    "        except:\n",
    "            indices.append(len(embedding_matrix)-1)  # Handle out-of-vocabulary words\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['word_indices'] = train_df['text'].apply(lambda x: tokens_to_indices(x, word2vec))\n",
    "test_df['word_indices'] = test_df['text'].apply(lambda x: tokens_to_indices(x, word2vec))\n",
    "development_df['word_indices'] = development_df['text'].apply(lambda x: tokens_to_indices(x, word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>text</th>\n",
       "      <th>word_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[How, did, serfdom, develop, in, and, then, le...</td>\n",
       "      <td>[1190, 92, 178836, 1306, 1, 3000000, 145, 785,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[What, films, featured, the, character, Popeye...</td>\n",
       "      <td>[470, 2485, 2723, 11, 1980, 65760, 8347]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[How, can, I, find, a, list, of, celebrities, ...</td>\n",
       "      <td>[1190, 50, 20, 359, 3000000, 711, 3000000, 666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[What, fowl, grabs, the, spotlight, after, the...</td>\n",
       "      <td>[470, 43119, 10673, 11, 6236, 55, 11, 1035, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[What, is, the, full, form, of, com]</td>\n",
       "      <td>[470, 4, 11, 335, 815, 3000000, 18117]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>1</td>\n",
       "      <td>[What, s, the, shape, of, a, camel, s, spine]</td>\n",
       "      <td>[470, 1280, 11, 2790, 3000000, 3000000, 28380,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>1</td>\n",
       "      <td>[What, type, of, currency, is, used, in, China]</td>\n",
       "      <td>[470, 1474, 3000000, 2325, 4, 233, 1, 367]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>4</td>\n",
       "      <td>[What, is, the, temperature, today]</td>\n",
       "      <td>[470, 4, 11, 4360, 205]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>4</td>\n",
       "      <td>[What, is, the, temperature, for, cooking]</td>\n",
       "      <td>[470, 4, 11, 4360, 2, 5195]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451</th>\n",
       "      <td>1</td>\n",
       "      <td>[What, currency, is, used, in, Australia]</td>\n",
       "      <td>[470, 2325, 4, 233, 1, 904]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4952 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label-coarse                                               text  \\\n",
       "0                0  [How, did, serfdom, develop, in, and, then, le...   \n",
       "1                1  [What, films, featured, the, character, Popeye...   \n",
       "2                0  [How, can, I, find, a, list, of, celebrities, ...   \n",
       "3                1  [What, fowl, grabs, the, spotlight, after, the...   \n",
       "4                2               [What, is, the, full, form, of, com]   \n",
       "...            ...                                                ...   \n",
       "5447             1      [What, s, the, shape, of, a, camel, s, spine]   \n",
       "5448             1    [What, type, of, currency, is, used, in, China]   \n",
       "5449             4                [What, is, the, temperature, today]   \n",
       "5450             4         [What, is, the, temperature, for, cooking]   \n",
       "5451             1          [What, currency, is, used, in, Australia]   \n",
       "\n",
       "                                           word_indices  \n",
       "0     [1190, 92, 178836, 1306, 1, 3000000, 145, 785,...  \n",
       "1              [470, 2485, 2723, 11, 1980, 65760, 8347]  \n",
       "2     [1190, 50, 20, 359, 3000000, 711, 3000000, 666...  \n",
       "3     [470, 43119, 10673, 11, 6236, 55, 11, 1035, 16...  \n",
       "4                [470, 4, 11, 335, 815, 3000000, 18117]  \n",
       "...                                                 ...  \n",
       "5447  [470, 1280, 11, 2790, 3000000, 3000000, 28380,...  \n",
       "5448         [470, 1474, 3000000, 2325, 4, 233, 1, 367]  \n",
       "5449                            [470, 4, 11, 4360, 205]  \n",
       "5450                        [470, 4, 11, 4360, 2, 5195]  \n",
       "5451                        [470, 2325, 4, 233, 1, 904]  \n",
       "\n",
       "[4952 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom dataset, collate function for dataloader and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data, targets = zip(*batch)\n",
    "    \n",
    "    # Sort the batch by sequence length (optional, but can improve efficiency)\n",
    "    sorted_indices = sorted(range(len(data)), key=lambda i: len(data[i]), reverse=True)\n",
    "    data = [data[i] for i in sorted_indices]\n",
    "    targets = [targets[i] for i in sorted_indices]\n",
    "\n",
    "    # Create a list of sequences and their corresponding lengths\n",
    "    sequences = [torch.tensor(seq) for seq in data]\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "    # Pad the sequences to the length of the longest sequence in the batch\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    # Create packed sequence for RNNs (optional, if you're using an RNN)\n",
    "    # packed_sequences = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    return padded_sequences, torch.tensor(targets), torch.tensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyCustomDataset(train_df['word_indices'].to_numpy(), train_df['label-coarse'].to_numpy())\n",
    "test_dataset = MyCustomDataset(test_df['word_indices'].to_numpy(), test_df['label-coarse'].to_numpy())\n",
    "development_dataset = MyCustomDataset(development_df['word_indices'].to_numpy(), development_df['label-coarse'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "development_dataloader = DataLoader(development_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Text Classifier Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, bidirectional, device, aggregate='last'):\n",
    "        super(LSTMTextClassifier, self).__init__()\n",
    "        self.aggregate = aggregate\n",
    "        # Embedding layer with pretrained word vectors\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix).to(device))\n",
    "        self.embedding.weight.requires_grad = False # freeze the embeddings\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        \n",
    "        # Softmax Layer\n",
    "        self.softmax_layer = nn.Sequential(nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim),\n",
    "        nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text.to(torch.int64))\n",
    "        \n",
    "        #Pack the embedded sequences to handle variable-length sequences\n",
    "        packed_embedded = pack_padded_sequence(embedded, text_lengths.cpu().to(torch.int64), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Pass through the LSTM layer\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "\n",
    "        # Unpack the packed sequences\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        if(self.aggregate == 'last'):\n",
    "            # Use the final hidden state as the representation for the sentence\n",
    "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1) if self.lstm.bidirectional else hidden[-1, :, :]\n",
    "            # Use max pooling to aggregate as the representation for the sentence\n",
    "        elif(self.aggregate == 'max'):\n",
    "            hidden, _ = torch.max(output, dim = 1)\n",
    "            # Use average pooling to aggregate as the representation for the sentence\n",
    "        elif(self.aggregate == 'average'):\n",
    "            hidden = torch.mean(output, dim = 1)\n",
    "        \n",
    "        # Pass through the softmax layer\n",
    "        output = self.softmax_layer(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "hidden_dim = 256\n",
    "output_dim = 5\n",
    "vocab_size = len(word2vec.index_to_key)\n",
    "embedding_dim = 300\n",
    "num_layer = 1\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, valid_dataloader, optimizer, loss_fn, device, num_epochs=50):\n",
    "    model.to(device)\n",
    "    dev_acc_per_epoch = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        # ======== training phase ==========\n",
    "        train_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            text, labels, text_lengths = batch\n",
    "            text, labels, text_lengths  = torch.tensor(text).to(device), labels.to(device), text_lengths\n",
    "\n",
    "            optimizer.zero_grad() # clear gradients\n",
    "\n",
    "            output = model(text, text_lengths)\n",
    "\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        # ====================================\n",
    "\n",
    "\n",
    "        # ========== validation phase =========\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_dataloader:\n",
    "                text, labels, text_lengths = batch\n",
    "                text, labels, text_lengths  = text.to(device), labels.to(device), text_lengths\n",
    "\n",
    "                output = model(text, text_lengths)\n",
    "                loss = loss_fn(output, labels)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average validation loss and accuracy for the epoch\n",
    "        avg_valid_loss = valid_loss / len(valid_dataloader)\n",
    "        accuracy = 100 * (correct / total)\n",
    "        dev_acc_per_epoch.append(accuracy)\n",
    "        # =======================================\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_valid_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    print('Training complete.')\n",
    "\n",
    "    return dev_acc_per_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataloader, device, loss_fn):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            text, labels, text_lengths = batch\n",
    "            text, labels, text_lengths  = text.to(device), labels.to(device), text_lengths\n",
    "\n",
    "            output = model(text, text_lengths)\n",
    "            loss = loss_fn(output, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average validation loss and accuracy for the epoch\n",
    "    avg_valid_loss = valid_loss / len(test_dataloader)\n",
    "    accuracy = 100 * (correct / total)\n",
    "\n",
    "    return avg_valid_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test model with aggregation as taking representation of last word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdzak\\AppData\\Local\\Temp\\ipykernel_14496\\1751617731.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix).to(device))\n"
     ]
    }
   ],
   "source": [
    "lstm_classifier_last = LSTMTextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim,num_layer,True, device, aggregate='last').to(device)\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(lstm_classifier_last.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdzak\\AppData\\Local\\Temp\\ipykernel_14496\\684474237.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  text, labels, text_lengths  = torch.tensor(text).to(device), labels.to(device), text_lengths\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape\n",
       "==========================================================================================\n",
       "LSTMTextClassifier                       [32, 12]                  [32, 5]\n",
       "├─Embedding: 1-1                         [32, 12]                  [32, 12, 300]\n",
       "├─LSTM: 1-2                              [241, 300]                [241, 512]\n",
       "├─Sequential: 1-3                        [32, 512]                 [32, 5]\n",
       "│    └─Linear: 2-1                       [32, 512]                 [32, 5]\n",
       "│    └─Softmax: 2-2                      [32, 5]                   [32, 5]\n",
       "==========================================================================================\n",
       "Total params: 901,145,649\n",
       "Trainable params: 1,145,349\n",
       "Non-trainable params: 900,000,300\n",
       "Total mult-adds (G): 169.81\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 1.91\n",
       "Params size (MB): 3604.58\n",
       "Estimated Total Size (MB): 3606.50\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"DOT\"] = \"C:\\Program Files\\Graphviz\\bin\\dot.exe\"\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    text, labels, text_lengths = batch\n",
    "    text, labels, text_lengths  = torch.tensor(text).to(device), labels.to(device), text_lengths\n",
    "    break\n",
    "\n",
    "summary(lstm_classifier_last, input_data=[text, text_lengths], col_names=['input_size', 'output_size'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_acc_per_epoch = train(lstm_classifier_last, train_dataloader, development_dataloader, optimizer, loss_fn, device, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "epochs = list(range(1, len(dev_acc_per_epoch)+1))\n",
    "plt.plot(epochs, dev_acc_per_epoch, marker='x', linestyle='-')\n",
    "plt.title('Test Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get test accuracy and loss for test datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_valid_loss, accuracy = test(lstm_classifier_last, test_dataloader, device, loss_fn)\n",
    "\n",
    "print(\"Avg test loss:\", avg_valid_loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test model with aggregation as max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_classifier_max = LSTMTextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim,num_layer,True, device, aggregate='max')\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(lstm_classifier_max.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_acc_per_epoch = train(lstm_classifier_max, train_dataloader, development_dataloader, optimizer, loss_fn, device, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "epochs = list(range(1, len(dev_acc_per_epoch)+1))\n",
    "plt.plot(epochs, dev_acc_per_epoch, marker='x', linestyle='-')\n",
    "plt.title('Test Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_valid_loss, accuracy = test(lstm_classifier_max, test_dataloader, device, loss_fn)\n",
    "\n",
    "print(\"Avg test loss:\", avg_valid_loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test model with aggregation as average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_classifier_avg = LSTMTextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim,num_layer,True, device, aggregate='average')\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(lstm_classifier_avg.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_acc_per_epoch = train(lstm_classifier_avg, train_dataloader, development_dataloader, optimizer, loss_fn, device, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "epochs = list(range(1, len(dev_acc_per_epoch)+1))\n",
    "plt.plot(epochs, dev_acc_per_epoch, marker='x', linestyle='-')\n",
    "plt.title('Test Accuracy Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_valid_loss, accuracy = test(lstm_classifier_avg, test_dataloader, device, loss_fn)\n",
    "\n",
    "print(\"Avg test loss:\", avg_valid_loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
