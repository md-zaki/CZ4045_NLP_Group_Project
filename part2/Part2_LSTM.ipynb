{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "85dff655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d0818cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process data \n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "train_data , development_data = train_test_split(train_data,test_size=500,random_state=True)\n",
    "train_data.drop('label-fine',axis=1)\n",
    "test_data.drop('label-fine',axis=1)\n",
    "development_data.drop('label-fine',axis=1)\n",
    "labels = [0, 1, 2, 3, 4, 5]\n",
    "selected_labels = random.sample(labels, 4)\n",
    "selected_labels.sort()\n",
    "train_data['label-coarse'] = train_data['label-coarse'].apply(lambda x: selected_labels.index(x) if x in selected_labels else 4)\n",
    "test_data['label-coarse'] = test_data['label-coarse'].apply(lambda x: selected_labels.index(x) if x in selected_labels else 4)\n",
    "development_data['label-coarse'] = development_data['label-coarse'].apply(lambda x: selected_labels.index(x) if x in selected_labels else 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e360b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_news = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9e7fae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (h_n, _) = self.rnn(embedded)\n",
    "        aggregated = h_n[-1]  \n",
    "        out = self.fc(aggregated) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d89ae4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters (Can be changed to maybe improve accuracy)\n",
    "hidden_dim = 512\n",
    "num_classes = 5\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6bdb7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform tokenization\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_sequences=[]\n",
    "development_sequences=[]\n",
    "test_sequences=[]\n",
    "for text in train_data['text']:\n",
    "    x=tokenizer(text)\n",
    "    train_sequences.append(x) \n",
    "for text in development_data['text']:\n",
    "    x=tokenizer(text)\n",
    "    development_sequences.append(x) \n",
    "for text in test_data['text']:\n",
    "    x=tokenizer(text)\n",
    "    test_sequences.append(x)\n",
    "\n",
    "y_train = train_data['label-coarse'].tolist()\n",
    "y_val = development_data['label-coarse'].tolist()\n",
    "y_test = test_data['label-coarse'].tolist()\n",
    "#Training the word2vec model(Replace with trained word2vec model from part 1)\n",
    "google_news_model = Word2Vec(train_sequences, vector_size=100, window=5, min_count=1)\n",
    "embedding_dim = google_news_model.wv.vector_size\n",
    "vocab_size = google_news_model.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9f3eb4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = torch.FloatTensor(google_news_model.wv.vectors)\n",
    "# Create an embedding layer\n",
    "embedding_layer = nn.Embedding(google_news_model.corpus_total_words+1,google_news_model.vector_size)\n",
    "unk_token = np.random.uniform(-1,1,100)\n",
    "embedding_layer.weight.data[-1] = torch.FloatTensor(unk_token)\n",
    "X_train = []\n",
    "X_val = []\n",
    "X_test = []\n",
    "max_size = 20\n",
    "#Perform zero padding because all parameters must be the same length for some reason\n",
    "for input_text in train_sequences:\n",
    "    input_indices = [google_news_model.wv.key_to_index.get(word,vocab_size-1) for word in input_text]\n",
    "    input_indices = (input_indices + [0] * 20)[:20]\n",
    "    X_train.append(input_indices)\n",
    "    \n",
    "for input_text in development_sequences:\n",
    "    input_indices = [google_news_model.wv.key_to_index.get(word,vocab_size-1) for word in input_text]\n",
    "    input_indices = (input_indices + [0] * 20)[:20]\n",
    "    X_val.append(input_indices)\n",
    "    \n",
    "for input_text in test_sequences:\n",
    "    input_indices = [google_news_model.wv.key_to_index.get(word,vocab_size-1) for word in input_text]\n",
    "    input_indices = (input_indices + [0] * 20)[:20]\n",
    "    X_test.append(input_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4f5d188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialise_loaders(X_train, y_train,X_dev,y_dev, X_test, y_test):\n",
    "    train_data = torch.utils.data.TensorDataset(torch.LongTensor(X_train), torch.LongTensor(y_train))\n",
    "    development_data = torch.utils.data.TensorDataset(torch.LongTensor(X_dev), torch.LongTensor(y_dev))\n",
    "    test_data = torch.utils.data.TensorDataset(torch.LongTensor(X_test), torch.LongTensor(y_test))\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    dev_dataloader = torch.utils.data.DataLoader(development_data, batch_size=batch_size) \n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)   \n",
    " \n",
    "    return train_dataloader,dev_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader,dev_dataloader, test_dataloader = intialise_loaders(X_train, y_train,X_val,y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1996c634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] Loss: 1.5947 Train Accuracy: 0.2347\n",
      "Epoch [1/50] Loss: 1.5284 Development Accuracy: 0.3660\n",
      "Epoch [2/50] Loss: 1.4040 Train Accuracy: 0.3924\n",
      "Epoch [2/50] Loss: 1.1720 Development Accuracy: 0.5200\n",
      "Epoch [3/50] Loss: 1.1092 Train Accuracy: 0.4994\n",
      "Epoch [3/50] Loss: 1.0551 Development Accuracy: 0.5360\n",
      "Epoch [4/50] Loss: 1.0191 Train Accuracy: 0.5380\n",
      "Epoch [4/50] Loss: 1.0486 Development Accuracy: 0.5620\n",
      "Epoch [5/50] Loss: 0.8609 Train Accuracy: 0.6254\n",
      "Epoch [5/50] Loss: 0.9649 Development Accuracy: 0.6020\n",
      "Epoch [6/50] Loss: 0.6912 Train Accuracy: 0.7405\n",
      "Epoch [6/50] Loss: 0.8278 Development Accuracy: 0.7260\n",
      "Epoch [7/50] Loss: 0.5176 Train Accuracy: 0.8213\n",
      "Epoch [7/50] Loss: 0.7709 Development Accuracy: 0.7440\n",
      "Epoch [8/50] Loss: 0.3617 Train Accuracy: 0.8772\n",
      "Epoch [8/50] Loss: 0.7011 Development Accuracy: 0.7800\n",
      "Epoch [9/50] Loss: 0.2666 Train Accuracy: 0.9124\n",
      "Epoch [9/50] Loss: 0.7230 Development Accuracy: 0.7800\n",
      "Epoch [10/50] Loss: 0.1885 Train Accuracy: 0.9412\n",
      "Epoch [10/50] Loss: 0.7659 Development Accuracy: 0.7740\n",
      "Epoch [11/50] Loss: 0.1327 Train Accuracy: 0.9647\n",
      "Epoch [11/50] Loss: 0.8153 Development Accuracy: 0.7700\n",
      "Early stopping after 11 epochs with no improvement.\n"
     ]
    }
   ],
   "source": [
    "#Initialize the model\n",
    "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "train_accuracies = []\n",
    "dev_accuracies = []\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "patience = 3\n",
    "best_accuracy = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  \n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _,predicted = torch.max(outputs.data, 1) \n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "    average_loss = running_loss / len(train_dataloader)\n",
    "    train_losses.append(average_loss)\n",
    "    train_accuracy = correct_train / total_train\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {average_loss:.4f} Train Accuracy: {train_accuracy:.4f}\")     \n",
    "    \n",
    "    model.eval()\n",
    "    correct_dev = 0\n",
    "    total_dev = 0\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for inputs, labels in dev_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            val_loss += nn.CrossEntropyLoss()(outputs, labels)\n",
    "            _,predicted = torch.max(outputs.data, 1) \n",
    "            correct_dev += (predicted == labels).sum().item()\n",
    "            total_dev += labels.size(0)\n",
    "    average_loss2 = val_loss / len(dev_dataloader)\n",
    "    dev_losses.append(average_loss2)    \n",
    "    dev_accuracy = correct_dev / total_dev\n",
    "    dev_accuracies.append(dev_accuracy)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {average_loss2:.4f} Development Accuracy: {dev_accuracy:.4f}\")\n",
    "    \n",
    "    if dev_accuracy > best_accuracy:\n",
    "        best_accuracy = dev_accuracy\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "\n",
    "    if no_improvement >= patience:\n",
    "        print(f\"Early stopping after {epoch + 1} epochs with no improvement.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3f904669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the model on test data\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            val_loss += nn.CrossEntropyLoss()(outputs, labels)\n",
    "            _,predicted = torch.max(outputs.data, 1) \n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "accuracy = correct_test / total_test\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "89d364b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 0, 2, 0, 1, 1, 2, 0, 2, 4, 4, 3, 2, 4, 0, 4, 4, 1, 4, 2, 4, 2, 4, 0,\n",
      "        3, 1, 4, 4, 3, 4, 4, 4, 4, 4, 3, 1, 2, 0, 4, 4, 0, 2, 0, 0, 4, 2, 4, 2,\n",
      "        2, 3, 1, 0, 1, 2, 3, 3, 2, 4, 2, 2, 1, 3, 2, 3, 4, 2, 4, 2, 0, 4, 4, 2,\n",
      "        2, 4, 4, 2, 0, 0, 2, 3, 4, 1, 2, 2, 2, 1, 2, 1, 2, 1, 0, 4, 2, 4, 4, 0,\n",
      "        3, 0, 4, 1, 2, 0, 4, 0, 4, 4, 0, 4, 4, 3, 0, 4, 0, 0, 2, 0, 4, 4, 0, 2,\n",
      "        3, 4, 4, 4, 1, 1, 4, 1, 2, 2, 2, 1, 2, 4, 1, 4, 0, 2, 4, 4, 4, 2, 2, 3,\n",
      "        0, 0, 2, 4, 2, 3, 4, 3, 2, 4, 1, 0, 3, 4, 0, 4, 4, 4, 1, 1, 2, 2, 0, 4,\n",
      "        3, 1, 4, 3, 4, 1, 0, 0, 3, 1, 4, 4, 4, 2, 4, 0, 1, 0, 3, 3, 1, 0, 2, 1,\n",
      "        2, 1, 0, 0, 0, 2, 4, 2, 0, 3, 4, 4, 0, 0, 3, 4, 3, 0, 4, 1, 4, 2, 4, 4,\n",
      "        0, 2, 0, 0, 4, 3, 0, 3, 1, 0, 0, 2, 3, 2, 3, 2, 4, 2, 4, 4, 2, 2, 2, 1,\n",
      "        0, 2, 2, 4])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "880fe8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 4, 2, 1, 1, 2, 0, 2, 4, 4, 3, 2, 4, 0, 4, 4, 1, 4, 4, 4, 2, 4, 0,\n",
      "        3, 1, 4, 4, 3, 4, 4, 4, 4, 4, 3, 0, 2, 0, 4, 4, 0, 2, 0, 0, 4, 2, 4, 2,\n",
      "        2, 3, 1, 3, 1, 2, 3, 3, 2, 4, 3, 4, 1, 3, 0, 3, 4, 2, 4, 0, 2, 4, 4, 2,\n",
      "        2, 4, 4, 2, 0, 0, 2, 3, 4, 0, 4, 2, 2, 0, 2, 1, 2, 1, 0, 4, 2, 4, 2, 3,\n",
      "        0, 0, 4, 1, 2, 2, 4, 0, 4, 4, 0, 2, 4, 2, 0, 0, 4, 3, 2, 0, 4, 4, 0, 2,\n",
      "        3, 4, 4, 4, 1, 1, 0, 1, 2, 2, 4, 1, 4, 4, 1, 4, 0, 2, 4, 4, 4, 2, 2, 3,\n",
      "        1, 0, 2, 4, 2, 3, 4, 3, 2, 4, 1, 0, 3, 4, 0, 4, 4, 4, 1, 1, 2, 3, 0, 4,\n",
      "        3, 0, 4, 3, 4, 1, 4, 0, 3, 1, 4, 4, 4, 2, 4, 0, 1, 0, 3, 0, 1, 0, 3, 1,\n",
      "        2, 0, 0, 0, 0, 2, 2, 2, 0, 3, 4, 4, 1, 0, 3, 2, 3, 0, 4, 1, 4, 4, 4, 4,\n",
      "        0, 2, 0, 0, 4, 3, 0, 3, 0, 0, 0, 4, 3, 4, 3, 2, 0, 0, 4, 4, 0, 2, 2, 1,\n",
      "        0, 2, 0, 4])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "df1d8db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 1, 4, 2, 2, 1, 0, 4, 4, 3, 1, 2, 1, 2, 2, 0, 1, 4, 2, 1, 4, 3, 4, 4, 1, 4, 3, 3, 3, 2, 3, 4, 2, 2, 2, 3, 2, 2, 2, 0, 4, 4, 4, 0, 0, 2, 4, 2, 1, 4, 1, 1, 4, 4, 4, 2, 3, 3, 2, 3, 1, 3, 0, 3, 1, 0, 4, 2, 3, 2, 2, 3, 1, 3, 4, 0, 2, 2, 1, 4, 2, 1, 2, 2, 4, 4, 1, 4, 3, 0, 4, 2, 2, 1, 3, 2, 4, 4, 0, 4, 1, 4, 1, 3, 4, 2, 0, 2, 3, 0, 4, 2, 1, 0, 1, 2, 0, 2, 3, 2, 2, 4, 1, 2, 1, 2, 2, 4, 2, 2, 1, 4, 0, 4, 0, 3, 1, 1, 4, 4, 0, 3, 0, 4, 0, 1, 0, 4, 4, 0, 1, 4, 2, 1, 3, 3, 4, 3, 3, 4, 4, 2, 3, 0, 3, 2, 4, 3, 1, 2, 2, 2, 4, 3, 3, 3, 2, 0, 2, 3, 4, 3, 4, 0, 1, 1, 4, 3, 0, 0, 2, 2, 2, 4, 3, 0, 1, 0, 0, 0, 3, 1, 0, 2, 3, 1, 2, 4, 4, 2, 2, 3, 3, 4, 0, 4, 0, 2, 4, 4, 2, 4, 0, 3, 0, 2, 4, 3, 1, 2, 1, 4, 2, 0, 4, 3, 4, 3, 2, 3, 4, 4, 4, 4, 3, 4, 4, 4, 0, 1, 3, 0, 0, 4, 3, 2, 0, 4, 2, 1, 1, 2, 0, 2, 4, 4, 3, 2, 4, 0, 4, 4, 1, 4, 4, 4, 2, 4, 0, 3, 1, 4, 4, 3, 4, 4, 4, 4, 4, 3, 0, 2, 0, 4, 4, 0, 2, 0, 0, 4, 2, 4, 2, 2, 3, 1, 3, 1, 2, 3, 3, 2, 4, 3, 4, 1, 3, 0, 3, 4, 2, 4, 0, 2, 4, 4, 2, 2, 4, 4, 2, 0, 0, 2, 3, 4, 0, 4, 2, 2, 0, 2, 1, 2, 1, 0, 4, 2, 4, 2, 3, 0, 0, 4, 1, 2, 2, 4, 0, 4, 4, 0, 2, 4, 2, 0, 0, 4, 3, 2, 0, 4, 4, 0, 2, 3, 4, 4, 4, 1, 1, 0, 1, 2, 2, 4, 1, 4, 4, 1, 4, 0, 2, 4, 4, 4, 2, 2, 3, 1, 0, 2, 4, 2, 3, 4, 3, 2, 4, 1, 0, 3, 4, 0, 4, 4, 4, 1, 1, 2, 3, 0, 4, 3, 0, 4, 3, 4, 1, 4, 0, 3, 1, 4, 4, 4, 2, 4, 0, 1, 0, 3, 0, 1, 0, 3, 1, 2, 0, 0, 0, 0, 2, 2, 2, 0, 3, 4, 4, 1, 0, 3, 2, 3, 0, 4, 1, 4, 4, 4, 4, 0, 2, 0, 0, 4, 3, 0, 3, 0, 0, 0, 4, 3, 4, 3, 2, 0, 0, 4, 4, 0, 2, 2, 1, 0, 2, 0, 4]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daef7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
