{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e33d3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9edd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the \"word2vec-google-news-300\" embeddings\n",
    "glove_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094b7c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar word to 'student': 'students' (Cosine Similarity: 0.7295)\n",
      "Most similar word to 'Apple': 'Apple_AAPL' (Cosine Similarity: 0.7457)\n",
      "Most similar word to 'apple': 'apples' (Cosine Similarity: 0.7204)\n"
     ]
    }
   ],
   "source": [
    "# Define the words to find similar words for\n",
    "words_to_find_similarities = [\"student\", \"Apple\", \"apple\"]\n",
    "\n",
    "# Initialize a dictionary to store the most similar words and their cosine similarities\n",
    "most_similar_words = {}\n",
    "\n",
    "# Find the most similar words and their cosine similarities for each word\n",
    "for word in words_to_find_similarities:\n",
    "    if word in glove_vectors:\n",
    "        # Use the model's `most_similar` function to find the most similar words\n",
    "        similar_words = glove_vectors.most_similar(word)\n",
    "        \n",
    "        # The most similar word and its cosine similarity are in the first result\n",
    "        most_similar_word, cosine_similarity = similar_words[0]\n",
    "        \n",
    "        # Store the most similar word and its cosine similarity in the dictionary\n",
    "        most_similar_words[word] = (most_similar_word, cosine_similarity)\n",
    "    else:\n",
    "        most_similar_words[word] = (\"Not in vocabulary\", 0.0)\n",
    "\n",
    "# Print the most similar words and their cosine similarities\n",
    "for word, (most_similar_word, cosine_similarity) in most_similar_words.items():\n",
    "    print(f\"Most similar word to '{word}': '{most_similar_word}' (Cosine Similarity: {cosine_similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07a07a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training file (number of sentences): 14986\n",
      "Size of the development file (number of sentences): 3465\n",
      "Size of the test file (number of sentences): 3683\n",
      "\n",
      "All possible word labels:\n",
      "Training Labels: ['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n",
      "Development Labels: ['B-MISC', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n",
      "Test Labels: ['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "def count_sentences(filename):\n",
    "    count = 0\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.strip() == \"\":\n",
    "                count += 1\n",
    "        # Increment count by 1 if the file doesn't end with an empty line\n",
    "#         if not line.endswith('\\n'):\n",
    "#             count += 1\n",
    "        if not line:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def extract_labels(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = file.read().strip()\n",
    "    \n",
    "    labels = set()\n",
    "    sentences = data.split('\\n\\n')\n",
    "    for sentence in sentences:\n",
    "        lines = sentence.split('\\n')\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) > 3:\n",
    "                label = parts[-1]\n",
    "                labels.add(label)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# File paths for the CoNLL-2003 dataset\n",
    "train_file = 'eng.train'\n",
    "dev_file = 'eng.testa'\n",
    "test_file = 'eng.testb'\n",
    "\n",
    "# Count the number of sentences in each file\n",
    "train_sentences = count_sentences(train_file)\n",
    "dev_sentences = count_sentences(dev_file)\n",
    "test_sentences = count_sentences(test_file)\n",
    "\n",
    "# Extract all possible word labels\n",
    "train_labels = extract_labels(train_file)\n",
    "dev_labels = extract_labels(dev_file)\n",
    "test_labels = extract_labels(test_file)\n",
    "\n",
    "print(f\"Size of the training file (number of sentences): {train_sentences}\")\n",
    "print(f\"Size of the development file (number of sentences): {dev_sentences}\")\n",
    "print(f\"Size of the test file (number of sentences): {test_sentences}\")\n",
    "\n",
    "print(\"\\nAll possible word labels:\")\n",
    "print(\"Training Labels:\", sorted(train_labels))\n",
    "print(\"Development Labels:\", sorted(dev_labels))\n",
    "print(\"Test Labels:\", sorted(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9447cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedding_matrix = glove_vectors.vectors\n",
    "embedding_matrix.shape\n",
    "\n",
    "zero_array = np.zeros((300,), dtype=float)\n",
    "embedding_matrix = np.vstack((embedding_matrix, zero_array)) # last element a zero array for OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e573fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(file_name):\n",
    "    # Initialize empty lists to store sentences and NER tags\n",
    "    sentences = []\n",
    "    tags = []\n",
    "\n",
    "    # Read the content of the CoNLL2003 file\n",
    "    with open(file_name, 'r') as file:\n",
    "        current_sentence = []  # Initialize an empty list for the current sentence\n",
    "        current_tags = []  # Initialize an empty list for the current NER tags\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:  # Empty line indicates the end of a sentence\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    tags.append(current_tags)\n",
    "                current_sentence = []  # Reset the current sentence\n",
    "                current_tags = []  # Reset the current NER tags\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                current_sentence.append(parts[0])\n",
    "                current_tags.append(parts[-1])\n",
    "\n",
    "    # Add the last sentence if there's no empty line after it\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        tags.append(current_tags)\n",
    "\n",
    "    # Print the result\n",
    "    # for sentence, tag in zip(sentences, tags):\n",
    "    #     print(sentence)\n",
    "    #     print(tag)\n",
    "\n",
    "    #print(tags)\n",
    "    \n",
    "    return sentences, tags\n",
    "\n",
    "def word_to_embedding(sentences, tags, padding_tag=\"O\"):\n",
    "    # Convert words to word embeddings\n",
    "    train_data = []\n",
    "    padded_tags = []\n",
    "\n",
    "    # Find the maximum sentence length\n",
    "    max_sentence_length = max(len(sentence) for sentence in sentences)\n",
    "\n",
    "    for sentence, tag_sequence in zip(sentences, tags):\n",
    "        sentence_embeddings = []\n",
    "        sentence_tags = []\n",
    "        for word, tag in zip(sentence, tag_sequence):\n",
    "            if word in glove_vectors:\n",
    "                embedding = glove_vectors[word]\n",
    "            else:\n",
    "                # Handle OOV words (e.g., use a zero vector)\n",
    "                embedding = np.zeros(300)  # Assuming 300 is the embedding dimension\n",
    "            sentence_embeddings.append(embedding)\n",
    "            sentence_tags.append(tag)\n",
    "\n",
    "        # Padding for both data and tags\n",
    "        while len(sentence_embeddings) < max_sentence_length:\n",
    "            sentence_embeddings.append(np.zeros(300))  # Zero padding for the remaining tokens\n",
    "            sentence_tags.append(padding_tag)\n",
    "\n",
    "        train_data.append(sentence_embeddings)\n",
    "        padded_tags.append(sentence_tags)\n",
    "\n",
    "    # Convert train_data and padded_tags to PyTorch tensors\n",
    "    train_data = torch.tensor(train_data)\n",
    "    \n",
    "    return train_data, padded_tags\n",
    "\n",
    "\n",
    "# def word_to_embedding(sentences,tags):\n",
    "#     # Convert words to word embeddings\n",
    "#     train_data = []\n",
    "#     for sentence in sentences:\n",
    "#         sentence_embeddings = []\n",
    "#         for word in sentence:\n",
    "#             if word in glove_vectors:\n",
    "#                 embedding = glove_vectors[word]\n",
    "#             else:\n",
    "#                 # Handle OOV words (e.g., use a zero vector)\n",
    "#                 embedding = np.zeros(300)  # Assuming 300 is the embedding dimension\n",
    "#             sentence_embeddings.append(embedding)\n",
    "\n",
    "#         train_data.append(sentence_embeddings)\n",
    "\n",
    "#     # Padding to ensure all sentences have the same length (if needed)\n",
    "#     max_sentence_length = max(len(sentence) for sentence in train_data)\n",
    "    \n",
    "#     for i, sentence in enumerate(train_data):\n",
    "#         while len(sentence) < max_sentence_length:\n",
    "#             sentence.append(np.zeros(300))  # Zero padding for the remaining tokens\n",
    "\n",
    "#     # Convert train_data to a PyTorch tensor\n",
    "#     train_data = torch.tensor(train_data)\n",
    "    \n",
    "#     return train_data, tags\n",
    "\n",
    "train_sentences, train_tags = tokenize(\"eng.train\")\n",
    "dev_sentences, dev_tags = tokenize(\"eng.testa\")\n",
    "test_sentences, test_tags = tokenize(\"eng.testb\")\n",
    "\n",
    "# train_data, train_labels = word_to_embedding(train_sentences, train_tags)\n",
    "# dev_data, dev_labels = word_to_embedding(dev_sentences, dev_tags)\n",
    "# test_data, test_labels = word_to_embedding(test_sentences, test_tags)\n",
    "# train_data = word_to_embedding(train_sentences)\n",
    "# dev_data = word_to_embedding(dev_sentences)\n",
    "# test_data = word_to_embedding(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4dbf9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_into_list(fileName):\n",
    "#     # Initialize empty lists for the first and last columns\n",
    "#     first_column = []\n",
    "#     last_column = []\n",
    "\n",
    "#     # Open and read the file\n",
    "#     with open(fileName, 'r') as file:\n",
    "#         for line in file:\n",
    "#             # Split the line into columns\n",
    "#             columns = line.strip().split()\n",
    "#             if columns:  # Check if the line is not empty\n",
    "#                 # Append the first and last columns to their respective lists\n",
    "#                 first_column.append(columns[0])\n",
    "#                 last_column.append(columns[-1])\n",
    "\n",
    "#     # Print the extracted lists\n",
    "# #     print(\"First Column (Words):\")\n",
    "# #     print(first_column)\n",
    "\n",
    "# #     print(\"\\nLast Column (Labels):\")\n",
    "# #     print(last_column)\n",
    "#     return first_column, last_column\n",
    "\n",
    "# train_data, train_labels = extract_into_list(train_file)\n",
    "# dev_data, dev_labels = extract_into_list(dev_file)\n",
    "# test_data, test_labels = extract_into_list(test_file)\n",
    "\n",
    "# def convert_to_embedding(train_data):\n",
    "#     # Convert words to word embeddings\n",
    "#     train_data_embeddings = []\n",
    "#     for word in train_data:\n",
    "#         if word in glove_vectors:\n",
    "#             # Check if the word is in the GloVe vocabulary\n",
    "#             embedding = glove_vectors[word]\n",
    "#             train_data_embeddings.append(embedding)\n",
    "#         else:\n",
    "#             # Handle out-of-vocabulary words (e.g., by using a special token or zero vector)\n",
    "#             train_data_embeddings.append(zero_array)\n",
    "#     return train_data_embeddings\n",
    "\n",
    "# train_data = convert_to_embedding(train_data)\n",
    "# dev_data = convert_to_embedding(dev_data)\n",
    "# test_data = convert_to_embedding(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b671311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "    \n",
    "batch_size = 32\n",
    "train_dataset = NERDataset(train_sentences, train_tags)\n",
    "dev_dataset = NERDataset(dev_sentences, dev_tags)\n",
    "test_dataset = NERDataset(test_sentences, test_tags)\n",
    "# train_dataset = NERDataset(train_data, train_labels)\n",
    "# dev_dataset = NERDataset(dev_data, dev_labels)\n",
    "# test_dataset = NERDataset(test_data, test_labels)\n",
    "# train_dataset = NERDataset(np.array(train_data, dtype=float), np.array(train_labels))\n",
    "# dev_dataset = NERDataset(np.array(dev_data, dtype=float), np.array(dev_labels))\n",
    "# test_dataset = NERDataset(np.array(test_data, dtype=float), np.array(test_labels))\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "dev_data_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle the dev set\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b78ffba",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     72\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_data_loader:\n\u001b[0;32m     75\u001b[0m         text, tags \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     76\u001b[0m         text \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(text)\n",
      "File \u001b[1;32m~\\Desktop\\nlp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\Desktop\\nlp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\Desktop\\nlp\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\nlp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\nlp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Desktop\\nlp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Desktop\\nlp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:138\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    136\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMNER(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super(LSTMNER, self).__init__()\n",
    "        \n",
    "        # Embedding layer (use pretrained word embeddings)\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)#nn.Embedding.from_pretrained(torch.tensor(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Output layer (softmax classifier)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        print(\"DEBUG...Text:\" , text.shape)\n",
    "        embedded = self.embedding(text.to(torch.long))\n",
    "        #print(embedded)\n",
    "        print(\"DEBUG:\" , embedded.shape)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(output)\n",
    "        return predictions\n",
    "\n",
    "# Define a function to evaluate the model\n",
    "def evaluate(model, data_loader, tag_vocab):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            text, tags = batch\n",
    "            text = torch.tensor(text)\n",
    "            predictions = model(text)\n",
    "            # Extract predicted labels and true labels, and convert them to human-readable labels\n",
    "            true_labels = data_loader.labels\n",
    "\n",
    "    f1 = f1_score(true_labels, predictions, average='micro')  # Adjust 'average' as needed\n",
    "    return f1\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "tag_vocab = ['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n",
    "input_dim = len(glove_vectors.index_to_key)\n",
    "embedding_dim = 300  # Use the dimension of your pretrained word embeddings\n",
    "hidden_dim = 256  # Adjust as needed\n",
    "output_dim = 8#len(tag_vocab)\n",
    "num_layers = 2  # Adjust as needed\n",
    "bidirectional = True  # You can change this based on your requirements\n",
    "dropout = 0.5  # Adjust as needed\n",
    "\n",
    "model = LSTMNER(input_dim, embedding_dim, hidden_dim, output_dim, num_layers, bidirectional, dropout)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)  # Adjust the learning rate as needed\n",
    "\n",
    "# Training loop\n",
    "best_f1 = 0.0\n",
    "patience = 3  # Number of epochs to wait for F1 improvement\n",
    "current_patience = 0\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_data_loader:\n",
    "        text, tags = batch\n",
    "        text = torch.tensor(text)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text)\n",
    "        #predictions = predictions.view(-1, output_dim)\n",
    "        #tags = tags.view(-1)\n",
    "        loss = criterion(predictions, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate on the development set\n",
    "    dev_f1 = evaluate(model, dev_data_loader, tag_vocab)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}]. Development F1: {dev_f1:.4f}')\n",
    "    \n",
    "    if dev_f1 > best_f1:\n",
    "        best_f1 = dev_f1\n",
    "        current_patience = 0\n",
    "        # Save the model checkpoint here if needed\n",
    "    else:\n",
    "        current_patience += 1\n",
    "        if current_patience >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_f1 = evaluate(model, test_data_loader, tag_vocab)\n",
    "print(f'Final Test F1: {test_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c3b1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
