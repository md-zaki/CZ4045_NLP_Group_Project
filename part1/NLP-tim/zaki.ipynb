{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the \"word2vec-google-news-300\" embeddings\n",
    "glove_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(file_name):\n",
    "    # Initialize empty lists to store sentences and NER tags\n",
    "    sentences = []\n",
    "    tags = []\n",
    "\n",
    "    # Read the content of the CoNLL2003 file\n",
    "    with open(file_name, 'r') as file:\n",
    "        current_sentence = []  # Initialize an empty list for the current sentence\n",
    "        current_tags = []  # Initialize an empty list for the current NER tags\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:  # Empty line indicates the end of a sentence\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    tags.append(current_tags)\n",
    "                current_sentence = []  # Reset the current sentence\n",
    "                current_tags = []  # Reset the current NER tags\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                current_sentence.append(parts[0])\n",
    "                current_tags.append(parts[-1])\n",
    "\n",
    "    # Add the last sentence if there's no empty line after it\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        tags.append(current_tags)\n",
    "\n",
    "    # Print the result\n",
    "    # for sentence, tag in zip(sentences, tags):\n",
    "    #     print(sentence)\n",
    "    #     print(tag)\n",
    "\n",
    "    #print(tags)\n",
    "    \n",
    "    return sentences, tags\n",
    "\n",
    "def extract_labels(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = file.read().strip()\n",
    "    \n",
    "    labels = set()\n",
    "    sentences = data.split('\\n\\n')\n",
    "    for sentence in sentences:\n",
    "        lines = sentence.split('\\n')\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) > 3:\n",
    "                label = parts[-1]\n",
    "                labels.add(label)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_labels = list(sorted(extract_labels(\"eng.train\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_tags = tokenize(\"eng.train\")\n",
    "dev_sentences, dev_tags = tokenize(\"eng.testa\")\n",
    "test_sentences, test_tags = tokenize(\"eng.testb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'text': train_sentences, 'labels': train_tags})\n",
    "df_dev = pd.DataFrame({'text': dev_sentences, 'labels': dev_tags})\n",
    "df_test = pd.DataFrame({'text': test_sentences, 'labels': test_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = glove_vectors.vectors\n",
    "zero_array = np.zeros((300,))\n",
    "embedding_matrix = np.vstack((embedding_matrix, zero_array)) # last element a zero array for OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_indices(tokens, model):\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            indices.append(model.get_index(token))\n",
    "        except:\n",
    "            indices.append(len(embedding_matrix)-1)  # Handle out-of-vocabulary words\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_indices(tokens):\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        indices.append(pos_train_labels.index(token))\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['word_indices'] = df_train['text'].apply(lambda x: tokens_to_indices(x, glove_vectors))\n",
    "df_dev['word_indices'] = df_dev['text'].apply(lambda x: tokens_to_indices(x, glove_vectors))\n",
    "df_test['word_indices'] = df_test['text'].apply(lambda x: tokens_to_indices(x, glove_vectors))\n",
    "\n",
    "df_train['label_indices'] = df_train['labels'].apply(lambda x: labels_to_indices(x))\n",
    "df_dev['label_indices'] = df_dev['labels'].apply(lambda x: labels_to_indices(x))\n",
    "df_test['label_indices'] = df_test['labels'].apply(lambda x: labels_to_indices(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final dataframe with indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>word_indices</th>\n",
       "      <th>label_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n",
       "      <td>[I-ORG, O, I-MISC, O, O, O, I-MISC, O, O]</td>\n",
       "      <td>[1611, 11500, 1760, 315, 3000000, 8059, 882, 1...</td>\n",
       "      <td>[5, 7, 4, 7, 7, 7, 4, 7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[I-PER, I-PER]</td>\n",
       "      <td>[1918, 9039]</td>\n",
       "      <td>[6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[I-LOC, O]</td>\n",
       "      <td>[24412, 3000000]</td>\n",
       "      <td>[3, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, European, Commission, said, on, Thursday...</td>\n",
       "      <td>[O, I-ORG, I-ORG, O, O, O, O, O, O, I-MISC, O,...</td>\n",
       "      <td>[7, 802, 1380, 9, 5, 224, 15, 10913, 8, 1760, ...</td>\n",
       "      <td>[7, 5, 5, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Germany, 's, representative, to, the, Europea...</td>\n",
       "      <td>[I-LOC, O, O, O, O, I-ORG, I-ORG, O, O, O, I-P...</td>\n",
       "      <td>[1420, 3000000, 3071, 3000000, 11, 802, 1745, ...</td>\n",
       "      <td>[3, 7, 7, 7, 7, 5, 5, 7, 7, 7, 6, 6, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14982</th>\n",
       "      <td>[Division, two]</td>\n",
       "      <td>[O, O]</td>\n",
       "      <td>[1747, 54]</td>\n",
       "      <td>[7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14983</th>\n",
       "      <td>[Plymouth, 2, Preston, 1]</td>\n",
       "      <td>[I-ORG, O, I-ORG, O]</td>\n",
       "      <td>[9487, 200, 8838, 165]</td>\n",
       "      <td>[5, 7, 5, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14984</th>\n",
       "      <td>[Division, three]</td>\n",
       "      <td>[O, O]</td>\n",
       "      <td>[1747, 80]</td>\n",
       "      <td>[7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14985</th>\n",
       "      <td>[Swansea, 1, Lincoln, 2]</td>\n",
       "      <td>[I-ORG, O, I-ORG, O]</td>\n",
       "      <td>[15741, 165, 3633, 200]</td>\n",
       "      <td>[5, 7, 5, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14986</th>\n",
       "      <td>[-DOCSTART-]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[3000000]</td>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14987 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      [EU, rejects, German, call, to, boycott, Briti...   \n",
       "1                                     [Peter, Blackburn]   \n",
       "2                                 [BRUSSELS, 1996-08-22]   \n",
       "3      [The, European, Commission, said, on, Thursday...   \n",
       "4      [Germany, 's, representative, to, the, Europea...   \n",
       "...                                                  ...   \n",
       "14982                                    [Division, two]   \n",
       "14983                          [Plymouth, 2, Preston, 1]   \n",
       "14984                                  [Division, three]   \n",
       "14985                           [Swansea, 1, Lincoln, 2]   \n",
       "14986                                       [-DOCSTART-]   \n",
       "\n",
       "                                                  labels  \\\n",
       "0              [I-ORG, O, I-MISC, O, O, O, I-MISC, O, O]   \n",
       "1                                         [I-PER, I-PER]   \n",
       "2                                             [I-LOC, O]   \n",
       "3      [O, I-ORG, I-ORG, O, O, O, O, O, O, I-MISC, O,...   \n",
       "4      [I-LOC, O, O, O, O, I-ORG, I-ORG, O, O, O, I-P...   \n",
       "...                                                  ...   \n",
       "14982                                             [O, O]   \n",
       "14983                               [I-ORG, O, I-ORG, O]   \n",
       "14984                                             [O, O]   \n",
       "14985                               [I-ORG, O, I-ORG, O]   \n",
       "14986                                                [O]   \n",
       "\n",
       "                                            word_indices  \\\n",
       "0      [1611, 11500, 1760, 315, 3000000, 8059, 882, 1...   \n",
       "1                                           [1918, 9039]   \n",
       "2                                       [24412, 3000000]   \n",
       "3      [7, 802, 1380, 9, 5, 224, 15, 10913, 8, 1760, ...   \n",
       "4      [1420, 3000000, 3071, 3000000, 11, 802, 1745, ...   \n",
       "...                                                  ...   \n",
       "14982                                         [1747, 54]   \n",
       "14983                             [9487, 200, 8838, 165]   \n",
       "14984                                         [1747, 80]   \n",
       "14985                            [15741, 165, 3633, 200]   \n",
       "14986                                          [3000000]   \n",
       "\n",
       "                                           label_indices  \n",
       "0                            [5, 7, 4, 7, 7, 7, 4, 7, 7]  \n",
       "1                                                 [6, 6]  \n",
       "2                                                 [3, 7]  \n",
       "3      [7, 5, 5, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, ...  \n",
       "4      [3, 7, 7, 7, 7, 5, 5, 7, 7, 7, 6, 6, 7, 7, 7, ...  \n",
       "...                                                  ...  \n",
       "14982                                             [7, 7]  \n",
       "14983                                       [5, 7, 5, 7]  \n",
       "14984                                             [7, 7]  \n",
       "14985                                       [5, 7, 5, 7]  \n",
       "14986                                                [7]  \n",
       "\n",
       "[14987 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>word_indices</th>\n",
       "      <th>label_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
       "      <td>[O, O, I-LOC, O, O, O, O, I-PER, O, O, O, O]</td>\n",
       "      <td>[62630, 3000000, 75008, 47508, 156520, 31952, ...</td>\n",
       "      <td>[7, 7, 3, 7, 7, 7, 7, 6, 7, 7, 7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>[I-PER, I-PER]</td>\n",
       "      <td>[166344, 772342]</td>\n",
       "      <td>[6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
       "      <td>[I-LOC, O, I-LOC, I-LOC, I-LOC, O]</td>\n",
       "      <td>[3000000, 3000000, 1217, 2658, 11668, 3000000]</td>\n",
       "      <td>[3, 7, 3, 3, 3, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
       "      <td>[I-LOC, O, O, O, O, O, I-MISC, I-MISC, O, O, O...</td>\n",
       "      <td>[922, 548, 11, 3000000, 3000000, 30, 2125, 209...</td>\n",
       "      <td>[3, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
       "      <td>[O, I-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[84, 367, 653, 30, 4446, 6796, 82, 1, 11, 110,...</td>\n",
       "      <td>[7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679</th>\n",
       "      <td>[\", It, was, the, joy, that, we, all, had, ove...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[3000000, 51, 10, 11, 5942, 3, 38, 52, 35, 63,...</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3680</th>\n",
       "      <td>[Charlton, managed, Ireland, for, 93, matches,...</td>\n",
       "      <td>[I-PER, O, I-LOC, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[13572, 1464, 2620, 2, 3000000, 2004, 3000000,...</td>\n",
       "      <td>[6, 7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>[He, guided, Ireland, to, two, successive, Wor...</td>\n",
       "      <td>[O, O, I-LOC, O, O, O, I-MISC, I-MISC, O, O, O...</td>\n",
       "      <td>[57, 7064, 2620, 3000000, 54, 8339, 796, 2094,...</td>\n",
       "      <td>[7, 7, 3, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3682</th>\n",
       "      <td>[The, lanky, former, Leeds, United, defender, ...</td>\n",
       "      <td>[O, O, O, I-ORG, I-ORG, O, O, O, O, O, I-LOC, ...</td>\n",
       "      <td>[7, 33489, 249, 7178, 1217, 4206, 92, 13, 109,...</td>\n",
       "      <td>[7, 7, 7, 5, 5, 7, 7, 7, 7, 7, 3, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3683</th>\n",
       "      <td>[-DOCSTART-]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[3000000]</td>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3684 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
       "1                                        [Nadim, Ladki]   \n",
       "2       [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
       "3     [Japan, began, the, defence, of, their, Asian,...   \n",
       "4     [But, China, saw, their, luck, desert, them, i...   \n",
       "...                                                 ...   \n",
       "3679  [\", It, was, the, joy, that, we, all, had, ove...   \n",
       "3680  [Charlton, managed, Ireland, for, 93, matches,...   \n",
       "3681  [He, guided, Ireland, to, two, successive, Wor...   \n",
       "3682  [The, lanky, former, Leeds, United, defender, ...   \n",
       "3683                                       [-DOCSTART-]   \n",
       "\n",
       "                                                 labels  \\\n",
       "0          [O, O, I-LOC, O, O, O, O, I-PER, O, O, O, O]   \n",
       "1                                        [I-PER, I-PER]   \n",
       "2                    [I-LOC, O, I-LOC, I-LOC, I-LOC, O]   \n",
       "3     [I-LOC, O, O, O, O, O, I-MISC, I-MISC, O, O, O...   \n",
       "4     [O, I-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "...                                                 ...   \n",
       "3679  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3680  [I-PER, O, I-LOC, O, O, O, O, O, O, O, O, O, O...   \n",
       "3681  [O, O, I-LOC, O, O, O, I-MISC, I-MISC, O, O, O...   \n",
       "3682  [O, O, O, I-ORG, I-ORG, O, O, O, O, O, I-LOC, ...   \n",
       "3683                                                [O]   \n",
       "\n",
       "                                           word_indices  \\\n",
       "0     [62630, 3000000, 75008, 47508, 156520, 31952, ...   \n",
       "1                                      [166344, 772342]   \n",
       "2        [3000000, 3000000, 1217, 2658, 11668, 3000000]   \n",
       "3     [922, 548, 11, 3000000, 3000000, 30, 2125, 209...   \n",
       "4     [84, 367, 653, 30, 4446, 6796, 82, 1, 11, 110,...   \n",
       "...                                                 ...   \n",
       "3679  [3000000, 51, 10, 11, 5942, 3, 38, 52, 35, 63,...   \n",
       "3680  [13572, 1464, 2620, 2, 3000000, 2004, 3000000,...   \n",
       "3681  [57, 7064, 2620, 3000000, 54, 8339, 796, 2094,...   \n",
       "3682  [7, 33489, 249, 7178, 1217, 4206, 92, 13, 109,...   \n",
       "3683                                          [3000000]   \n",
       "\n",
       "                                          label_indices  \n",
       "0                  [7, 7, 3, 7, 7, 7, 7, 6, 7, 7, 7, 7]  \n",
       "1                                                [6, 6]  \n",
       "2                                    [3, 7, 3, 3, 3, 7]  \n",
       "3     [3, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "4     [7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "...                                                 ...  \n",
       "3679  [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "3680  [6, 7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "3681  [7, 7, 3, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 4, ...  \n",
       "3682  [7, 7, 7, 5, 5, 7, 7, 7, 7, 7, 3, 7, 7, 7, 7, ...  \n",
       "3683                                                [7]  \n",
       "\n",
       "[3684 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>word_indices</th>\n",
       "      <th>label_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CRICKET, -, LEICESTERSHIRE, TAKE, OVER, AT, T...</td>\n",
       "      <td>[O, O, I-ORG, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[93620, 3000000, 1552803, 71662, 36028, 11030,...</td>\n",
       "      <td>[7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[LONDON, 1996-08-30]</td>\n",
       "      <td>[I-LOC, O]</td>\n",
       "      <td>[4949, 3000000]</td>\n",
       "      <td>[3, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[West, Indian, all-rounder, Phil, Simmons, too...</td>\n",
       "      <td>[I-MISC, I-MISC, O, I-PER, I-PER, O, O, O, O, ...</td>\n",
       "      <td>[611, 1106, 3000000, 4811, 8262, 263, 134, 2, ...</td>\n",
       "      <td>[4, 4, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 5, 7, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Their, stay, on, top, ,, though, ,, may, be, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, I-ORG,...</td>\n",
       "      <td>[1697, 820, 5, 213, 3000000, 459, 3000000, 137...</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 5, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[After, bowling, Somerset, out, for, 83, on, t...</td>\n",
       "      <td>[O, O, I-ORG, O, O, O, O, O, O, O, O, I-LOC, I...</td>\n",
       "      <td>[361, 6842, 11008, 49, 2, 3000000, 5, 11, 798,...</td>\n",
       "      <td>[7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 7, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>[Brokers, said, blue, chips, like, IDLC, ,, Ba...</td>\n",
       "      <td>[O, O, O, O, O, I-ORG, O, I-ORG, I-ORG, O, I-O...</td>\n",
       "      <td>[29281, 9, 2836, 5695, 87, 1325185, 3000000, 5...</td>\n",
       "      <td>[7, 7, 7, 7, 7, 5, 7, 5, 5, 7, 5, 5, 7, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>[They, said, there, was, still, demand, for, b...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[128, 9, 72, 10, 151, 687, 2, 2836, 5695, 1, 2...</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>[The, DSE, all, share, price, index, closed, 2...</td>\n",
       "      <td>[O, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[7, 78228, 52, 306, 422, 1542, 866, 3000000, 1...</td>\n",
       "      <td>[7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>[--, Dhaka, Newsroom, 880-2-506363]</td>\n",
       "      <td>[O, I-ORG, I-ORG, O]</td>\n",
       "      <td>[3000000, 15087, 53872, 3000000]</td>\n",
       "      <td>[7, 5, 5, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>[-DOCSTART-]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[3000000]</td>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3466 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     [CRICKET, -, LEICESTERSHIRE, TAKE, OVER, AT, T...   \n",
       "1                                  [LONDON, 1996-08-30]   \n",
       "2     [West, Indian, all-rounder, Phil, Simmons, too...   \n",
       "3     [Their, stay, on, top, ,, though, ,, may, be, ...   \n",
       "4     [After, bowling, Somerset, out, for, 83, on, t...   \n",
       "...                                                 ...   \n",
       "3461  [Brokers, said, blue, chips, like, IDLC, ,, Ba...   \n",
       "3462  [They, said, there, was, still, demand, for, b...   \n",
       "3463  [The, DSE, all, share, price, index, closed, 2...   \n",
       "3464                [--, Dhaka, Newsroom, 880-2-506363]   \n",
       "3465                                       [-DOCSTART-]   \n",
       "\n",
       "                                                 labels  \\\n",
       "0                 [O, O, I-ORG, O, O, O, O, O, O, O, O]   \n",
       "1                                            [I-LOC, O]   \n",
       "2     [I-MISC, I-MISC, O, I-PER, I-PER, O, O, O, O, ...   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, I-ORG,...   \n",
       "4     [O, O, I-ORG, O, O, O, O, O, O, O, O, I-LOC, I...   \n",
       "...                                                 ...   \n",
       "3461  [O, O, O, O, O, I-ORG, O, I-ORG, I-ORG, O, I-O...   \n",
       "3462  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3463  [O, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "3464                               [O, I-ORG, I-ORG, O]   \n",
       "3465                                                [O]   \n",
       "\n",
       "                                           word_indices  \\\n",
       "0     [93620, 3000000, 1552803, 71662, 36028, 11030,...   \n",
       "1                                       [4949, 3000000]   \n",
       "2     [611, 1106, 3000000, 4811, 8262, 263, 134, 2, ...   \n",
       "3     [1697, 820, 5, 213, 3000000, 459, 3000000, 137...   \n",
       "4     [361, 6842, 11008, 49, 2, 3000000, 5, 11, 798,...   \n",
       "...                                                 ...   \n",
       "3461  [29281, 9, 2836, 5695, 87, 1325185, 3000000, 5...   \n",
       "3462  [128, 9, 72, 10, 151, 687, 2, 2836, 5695, 1, 2...   \n",
       "3463  [7, 78228, 52, 306, 422, 1542, 866, 3000000, 1...   \n",
       "3464                   [3000000, 15087, 53872, 3000000]   \n",
       "3465                                          [3000000]   \n",
       "\n",
       "                                          label_indices  \n",
       "0                     [7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7]  \n",
       "1                                                [3, 7]  \n",
       "2     [4, 4, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 5, 7, 5, ...  \n",
       "3     [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 5, 7, ...  \n",
       "4     [7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 7, 5, ...  \n",
       "...                                                 ...  \n",
       "3461  [7, 7, 7, 7, 7, 5, 7, 5, 5, 7, 5, 5, 7, 5, 5, ...  \n",
       "3462  [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "3463  [7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "3464                                       [7, 5, 5, 7]  \n",
       "3465                                                [7]  \n",
       "\n",
       "[3466 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create torch datasets and dataloader from DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data, targets = zip(*batch)\n",
    "    \n",
    "    # Sort the batch by sequence length (optional, but can improve efficiency)\n",
    "    sorted_indices = sorted(range(len(data)), key=lambda i: len(data[i]), reverse=True)\n",
    "    data = [data[i] for i in sorted_indices]\n",
    "    targets = [torch.tensor(targets[i]) for i in sorted_indices]\n",
    "\n",
    "    # Create a list of sequences and their corresponding lengths\n",
    "    sequences = [torch.tensor(seq) for seq in data]\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "    # Pad the sequences to the length of the longest sequence in the batch\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=7)\n",
    "\n",
    "    # Create packed sequence for RNNs (optional, if you're using an RNN)\n",
    "    # packed_sequences = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    return padded_sequences, padded_targets, torch.tensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyCustomDataset(df_train['word_indices'].to_numpy(), df_train['label_indices'].to_numpy())\n",
    "test_dataset = MyCustomDataset(df_test['word_indices'].to_numpy(), df_test['label_indices'].to_numpy())\n",
    "dev_dataset = MyCustomDataset(df_dev['word_indices'].to_numpy(), df_dev['label_indices'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "development_dataloader = DataLoader(dev_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, bidirectional, device):\n",
    "        super(LSTMTextClassifier, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.device = device\n",
    "        # Embedding layer with pretrained word vectors\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix).to(device))\n",
    "        self.embedding.weight.requires_grad = False # freeze the embeddings\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        \n",
    "        # # Softmax Layer\n",
    "        self.softmax_layer = nn.Linear(hidden_dim, self.output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text.to(torch.int64))\n",
    "        \n",
    "        #Pack the embedded sequences to handle variable-length sequences\n",
    "        packed_embedded = pack_padded_sequence(embedded, text_lengths.cpu().to(torch.int64), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Pass through the LSTM layer\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "\n",
    "        # Unpack the packed sequences\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        # Pass through the softmax layer\n",
    "        output = self.softmax_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "hidden_dim = 256\n",
    "output_dim = 8\n",
    "vocab_size = len(glove_vectors.index_to_key)\n",
    "embedding_dim = 300\n",
    "num_layer = 1\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, valid_dataloader, optimizer, loss_fn, device, num_epochs=50):\n",
    "    model.to(device)\n",
    "    dev_f1_per_epoch = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        # ======== training phase ==========\n",
    "        train_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            text, labels, text_lengths = batch\n",
    "            text, labels, text_lengths  = torch.tensor(text).to(device), labels.to(device), text_lengths\n",
    "\n",
    "            optimizer.zero_grad() # clear gradients\n",
    "\n",
    "            output = model(text, text_lengths)\n",
    "            \n",
    "            output = output.view(-1, 8)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        # ====================================\n",
    "\n",
    "\n",
    "        # ========== validation phase =========\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        f1 = 0\n",
    "\n",
    "        predictions = []\n",
    "        label_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_dataloader:\n",
    "                text, labels, text_lengths = batch\n",
    "                text, labels, text_lengths  = text.to(device), labels.to(device), text_lengths\n",
    "\n",
    "                output = model(text, text_lengths)\n",
    "                bs, sl, inp = output.size()\n",
    "                output = output.view(-1, 8)\n",
    "                labels = labels.view(-1)\n",
    "                loss = loss_fn(output, labels)\n",
    "                output = output.view(bs, sl, inp)\n",
    "                labels = labels.view(bs, sl)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output, 2)\n",
    "                for i in range(predicted.size(0)):\n",
    "                    pred = []\n",
    "                    lbl = []\n",
    "                    for j in predicted[i]:\n",
    "                        pred.append(pos_train_labels[j])\n",
    "                    for k in labels[i]:\n",
    "                        lbl.append(pos_train_labels[k])\n",
    "                    predictions.append(pred)\n",
    "                    label_list.append(lbl)\n",
    "\n",
    "                # print(\"output:\", output.shape)\n",
    "                # print(\"predicted:\", predicted.shape)\n",
    "                # print(\"labels:\", labels.shape)\n",
    "                # f1 = f1 + f1_score(labels, predicted)\n",
    "                # total += labels.size(0)\n",
    "                # correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "        # Calculate average validation loss and accuracy for the epoch\n",
    "        avg_valid_loss = valid_loss / len(valid_dataloader)\n",
    "        # print(\"label list:\", label_list)\n",
    "        # print(\"predictions:\",predictions)\n",
    "        f1 = f1_score(label_list, predictions)\n",
    "        dev_f1_per_epoch.append(f1)\n",
    "        # =======================================\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_valid_loss:.4f}, F1: {f1:.2f}%')\n",
    "\n",
    "    print('Training complete.')\n",
    "\n",
    "    return dev_f1_per_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataloader, device, loss_fn):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    label_list = []\n",
    "    with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                text, labels, text_lengths = batch\n",
    "                text, labels, text_lengths  = text.to(device), labels.to(device), text_lengths\n",
    "\n",
    "                output = model(text, text_lengths)\n",
    "                bs, sl, inp = output.size()\n",
    "                output = output.view(-1, 8)\n",
    "                labels = labels.view(-1)\n",
    "                loss = loss_fn(output, labels)\n",
    "                output = output.view(bs, sl, inp)\n",
    "                labels = labels.view(bs, sl)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output, 2)\n",
    "                for i in range(predicted.size(0)):\n",
    "                    pred = []\n",
    "                    lbl = []\n",
    "                    for j in predicted[i]:\n",
    "                        pred.append(pos_train_labels[j])\n",
    "                    for k in labels[i]:\n",
    "                        lbl.append(pos_train_labels[k])\n",
    "                    predictions.append(pred)\n",
    "                    label_list.append(lbl)\n",
    "\n",
    "                # print(\"output:\", output.shape)\n",
    "                # print(\"predicted:\", predicted.shape)\n",
    "                # print(\"labels:\", labels.shape)\n",
    "                # f1 = f1 + f1_score(labels, predicted)\n",
    "                # total += labels.size(0)\n",
    "                # correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    # Calculate average validation loss and accuracy for the epoch\n",
    "    avg_valid_loss = valid_loss / len(test_dataloader)\n",
    "    # print(\"label list:\", label_list)\n",
    "    # print(\"predictions:\",predictions)\n",
    "    f1 = f1_score(label_list, predictions)\n",
    "\n",
    "    return avg_valid_loss, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdzak\\AppData\\Local\\Temp\\ipykernel_6048\\4238509672.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix).to(device))\n"
     ]
    }
   ],
   "source": [
    "lstm_classifier = LSTMTextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim,num_layer,False, device).to(device)\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(lstm_classifier.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdzak\\AppData\\Local\\Temp\\ipykernel_6048\\1985713416.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  text, labels, text_lengths  = torch.tensor(text).to(device), labels.to(device), text_lengths\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 1.2068, Validation Loss: 0.8987, F1: 0.71%\n",
      "Epoch 2/50, Train Loss: 0.7339, Validation Loss: 0.5707, F1: 0.76%\n",
      "Epoch 3/50, Train Loss: 0.4661, Validation Loss: 0.3655, F1: 0.78%\n",
      "Epoch 4/50, Train Loss: 0.3035, Validation Loss: 0.2450, F1: 0.79%\n",
      "Epoch 5/50, Train Loss: 0.2074, Validation Loss: 0.1743, F1: 0.80%\n",
      "Epoch 6/50, Train Loss: 0.1489, Validation Loss: 0.1313, F1: 0.81%\n",
      "Epoch 7/50, Train Loss: 0.1117, Validation Loss: 0.1038, F1: 0.81%\n",
      "Epoch 8/50, Train Loss: 0.0866, Validation Loss: 0.0856, F1: 0.82%\n",
      "Epoch 9/50, Train Loss: 0.0689, Validation Loss: 0.0734, F1: 0.82%\n",
      "Epoch 10/50, Train Loss: 0.0558, Validation Loss: 0.0654, F1: 0.82%\n",
      "Epoch 11/50, Train Loss: 0.0458, Validation Loss: 0.0601, F1: 0.82%\n",
      "Epoch 12/50, Train Loss: 0.0378, Validation Loss: 0.0574, F1: 0.82%\n",
      "Epoch 13/50, Train Loss: 0.0315, Validation Loss: 0.0553, F1: 0.82%\n",
      "Epoch 14/50, Train Loss: 0.0268, Validation Loss: 0.0537, F1: 0.82%\n",
      "Epoch 15/50, Train Loss: 0.0228, Validation Loss: 0.0533, F1: 0.81%\n",
      "Epoch 16/50, Train Loss: 0.0197, Validation Loss: 0.0548, F1: 0.81%\n",
      "Epoch 17/50, Train Loss: 0.0167, Validation Loss: 0.0562, F1: 0.80%\n",
      "Epoch 18/50, Train Loss: 0.0149, Validation Loss: 0.0541, F1: 0.81%\n",
      "Epoch 19/50, Train Loss: 0.0133, Validation Loss: 0.0551, F1: 0.81%\n",
      "Epoch 20/50, Train Loss: 0.0122, Validation Loss: 0.0553, F1: 0.82%\n",
      "Epoch 21/50, Train Loss: 0.0113, Validation Loss: 0.0550, F1: 0.82%\n",
      "Epoch 22/50, Train Loss: 0.0103, Validation Loss: 0.0552, F1: 0.82%\n",
      "Epoch 23/50, Train Loss: 0.0098, Validation Loss: 0.0561, F1: 0.82%\n",
      "Epoch 24/50, Train Loss: 0.0093, Validation Loss: 0.0563, F1: 0.82%\n",
      "Epoch 25/50, Train Loss: 0.0089, Validation Loss: 0.0558, F1: 0.82%\n",
      "Epoch 26/50, Train Loss: 0.0085, Validation Loss: 0.0568, F1: 0.82%\n",
      "Epoch 27/50, Train Loss: 0.0081, Validation Loss: 0.0576, F1: 0.82%\n",
      "Epoch 28/50, Train Loss: 0.0079, Validation Loss: 0.0572, F1: 0.82%\n",
      "Epoch 29/50, Train Loss: 0.0077, Validation Loss: 0.0586, F1: 0.82%\n",
      "Epoch 30/50, Train Loss: 0.0077, Validation Loss: 0.0580, F1: 0.82%\n",
      "Epoch 31/50, Train Loss: 0.0081, Validation Loss: 0.0578, F1: 0.82%\n",
      "Epoch 32/50, Train Loss: 0.0077, Validation Loss: 0.0573, F1: 0.82%\n",
      "Epoch 33/50, Train Loss: 0.0083, Validation Loss: 0.0562, F1: 0.82%\n",
      "Epoch 34/50, Train Loss: 0.0079, Validation Loss: 0.0568, F1: 0.82%\n",
      "Epoch 35/50, Train Loss: 0.0071, Validation Loss: 0.0575, F1: 0.83%\n",
      "Epoch 36/50, Train Loss: 0.0069, Validation Loss: 0.0577, F1: 0.82%\n",
      "Epoch 37/50, Train Loss: 0.0069, Validation Loss: 0.0586, F1: 0.82%\n",
      "Epoch 38/50, Train Loss: 0.0069, Validation Loss: 0.0594, F1: 0.82%\n",
      "Epoch 39/50, Train Loss: 0.0068, Validation Loss: 0.0600, F1: 0.82%\n",
      "Epoch 40/50, Train Loss: 0.0068, Validation Loss: 0.0606, F1: 0.82%\n",
      "Epoch 41/50, Train Loss: 0.0068, Validation Loss: 0.0604, F1: 0.82%\n",
      "Epoch 42/50, Train Loss: 0.0067, Validation Loss: 0.0610, F1: 0.82%\n",
      "Epoch 43/50, Train Loss: 0.0068, Validation Loss: 0.0606, F1: 0.82%\n",
      "Epoch 44/50, Train Loss: 0.0069, Validation Loss: 0.0589, F1: 0.83%\n",
      "Epoch 45/50, Train Loss: 0.0070, Validation Loss: 0.0591, F1: 0.82%\n",
      "Epoch 46/50, Train Loss: 0.0070, Validation Loss: 0.0585, F1: 0.82%\n",
      "Epoch 47/50, Train Loss: 0.0067, Validation Loss: 0.0591, F1: 0.83%\n",
      "Epoch 48/50, Train Loss: 0.0065, Validation Loss: 0.0597, F1: 0.83%\n",
      "Epoch 49/50, Train Loss: 0.0068, Validation Loss: 0.0618, F1: 0.82%\n",
      "Epoch 50/50, Train Loss: 0.0066, Validation Loss: 0.0595, F1: 0.83%\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "dev_acc_per_epoch = train(lstm_classifier, train_dataloader, development_dataloader, optimizer, loss_fn, device, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 0.10690798055669613\n",
      "F1 Score: 0.7577499129223267\n"
     ]
    }
   ],
   "source": [
    "avg_valid_loss, f1 = test(lstm_classifier, test_dataloader, device, loss_fn)\n",
    "\n",
    "print(\"Avg test loss:\", avg_valid_loss)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
