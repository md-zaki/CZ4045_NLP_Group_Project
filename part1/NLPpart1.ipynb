{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP Project Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/burntpie/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Collecting gensim\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/7b/ef/d559c7daebb2f00b881575551b23866ebcbf6eeaf33393d692c7f46d0983/gensim-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading gensim-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/burntpie/miniconda3/envs/tf/lib/python3.9/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/burntpie/miniconda3/envs/tf/lib/python3.9/site-packages (from gensim) (1.11.1)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Obtaining dependency information for smart-open>=1.8.1 from https://files.pythonhosted.org/packages/fc/d9/d97f1db64b09278aba64e8c81b5d322d436132df5741c518f3823824fae0/smart_open-6.4.0-py3-none-any.whl.metadata\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading gensim-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-6.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the 'word2vec-google-news-300' embeddings\n",
    "word2vec_google = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: student\n",
      "Most similar word: students\n",
      "Cosine similarity: 0.7294867038726807\n",
      "--------\n",
      "Word: Apple\n",
      "Most similar word: Apple_AAPL\n",
      "Cosine similarity: 0.7456986308097839\n",
      "--------\n",
      "Word: apple\n",
      "Most similar word: apples\n",
      "Cosine similarity: 0.720359742641449\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "words = [\"student\", \"Apple\", \"apple\"]\n",
    "for word in words:\n",
    "    similar_word, similarity_score = word2vec_google.most_similar(word, topn=1)[0]\n",
    "    print(f\"Word: {word}\")\n",
    "    print(f\"Most similar word: {similar_word}\")\n",
    "    print(f\"Cosine similarity: {similarity_score}\")\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_file(file_path):\n",
    "    sentences = []  # This will store lists of word-label pairs, one list for each sentence\n",
    "    sentence = []   # Temporary list to store word-label pairs for the current sentence\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()  # Remove any leading/trailing whitespace\n",
    "            if line:  # If the line isn't empty (i.e., we're within a sentence)\n",
    "                word, _, _, label = line.split()  # Split the line to get the word and its associated label\n",
    "                sentence.append((word, label))\n",
    "            else:  # An empty line means the end of the current sentence\n",
    "                sentences.append(sentence)\n",
    "                sentence = []  # Reset the temporary list for the next sentence\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in training data: 14986\n",
      "Number of sentences in development data: 3465\n",
      "Number of sentences in test data: 3683\n",
      "All possible labels: {'B-LOC', 'B-MISC', 'I-PER', 'I-ORG', 'B-ORG', 'I-LOC', 'I-MISC', 'O'}\n"
     ]
    }
   ],
   "source": [
    "train_data = read_conll_file(\"eng.train\")\n",
    "dev_data = read_conll_file(\"eng.testa\")\n",
    "test_data = read_conll_file(\"eng.testb\")\n",
    "\n",
    "print(f\"Number of sentences in training data: {len(train_data)}\")\n",
    "print(f\"Number of sentences in development data: {len(dev_data)}\")\n",
    "print(f\"Number of sentences in test data: {len(test_data)}\")\n",
    "\n",
    "# Extract all unique labels from training data\n",
    "labels = set()\n",
    "for sentence in train_data:\n",
    "    for _, label in sentence:\n",
    "        labels.add(label)\n",
    "\n",
    "print(f\"All possible labels: {labels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .\n",
      "Named Entities: ['European Union', 'Werner Zwingmann']\n"
     ]
    }
   ],
   "source": [
    "for sentence in train_data:\n",
    "    entities = []          # List to store identified entities\n",
    "    current_entity = []    # Temporary list to store words of the current entity\n",
    "    current_label = None   # Track the label of the current entity\n",
    "\n",
    "    for word, label in sentence:\n",
    "        if label != 'O':  # If the word is part of an entity\n",
    "            prefix, label_type = label.split('-')  # Split the label into its prefix (B/I) and type (e.g., ORG, PER)\n",
    "            \n",
    "            # If it's the beginning of an entity or a continuation of a different entity type\n",
    "            if prefix == 'B' or (prefix == 'I' and current_label != label_type):\n",
    "                if current_entity:  # If we've been capturing an entity, add it to our list\n",
    "                    entities.append((' '.join(current_entity), current_label))\n",
    "                current_entity = [word]  # Start a new entity\n",
    "                current_label = label_type\n",
    "            elif prefix == 'I' and current_label == label_type:  # If it's a continuation of the same entity type\n",
    "                current_entity.append(word)\n",
    "        else:  # If the word is not part of any entity\n",
    "            if current_entity:  # If we've been capturing an entity, add it to our list and reset\n",
    "                entities.append((' '.join(current_entity), current_label))\n",
    "                current_entity = []\n",
    "                current_label = None\n",
    "\n",
    "    # Check if sentence matches criteria of having at least two multi-word named entities\n",
    "    entity_texts = [entity[0] for entity in entities if ' ' in entity[0]]\n",
    "    if len(entity_texts) >= 2:\n",
    "        print(f\"Sentence: {' '.join([word for word, _ in sentence])}\")\n",
    "        print(f\"Named Entities: {entity_texts}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec_google\n",
    "word2vec_google = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll2003_file(filepath):\n",
    "    \"\"\"Reads the CoNLL2003 file and returns sentences with their corresponding tags.\"\"\"\n",
    "    sentences = []\n",
    "    sentence_words = []\n",
    "    sentence_tags = []\n",
    "    \n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                # New sentence starts\n",
    "                if sentence_words and sentence_tags:\n",
    "                    sentences.append((sentence_words, sentence_tags))\n",
    "                    sentence_words = []\n",
    "                    sentence_tags = []\n",
    "            else:\n",
    "                # Extracting word and NER tag\n",
    "                parts = line.split(' ')\n",
    "                word = parts[0]\n",
    "                tag = parts[-1]\n",
    "                sentence_words.append(word)\n",
    "                sentence_tags.append(tag)\n",
    "                \n",
    "    if sentence_words and sentence_tags:\n",
    "        sentences.append((sentence_words, sentence_tags))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Example usage:\n",
    "train_data = read_conll2003_file(\"eng.train\")\n",
    "dev_data = read_conll2003_file(\"eng.testa\")\n",
    "test_data = read_conll2003_file(\"eng.testb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of all unique words and tags in the training dataset\n",
    "words = list(set(word for sentence, _ in train_data for word in sentence))\n",
    "tags = list(set(tag for _, tags in train_data for tag in tags))\n",
    "\n",
    "# Define your word_to_ix based on your dataset \n",
    "word_to_ix = {word: i for i, word in enumerate(words)}\n",
    "word_to_ix[\"UNK\"] = len(word_to_ix) # Add 'UNK' token\n",
    "word_to_ix[\"PAD\"] = len(word_to_ix) # Add 'PAD' token\n",
    "\n",
    "# Create word and tag dictionaries for mapping\n",
    "tag_to_ix = {tag: i for i, tag in enumerate(tags)}\n",
    "ix_to_tag = {i: tag for tag, i in tag_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ix_to_tag = {0: 'B-LOC', 1: 'B-MISC', 2: 'I-PER', 3: 'I-ORG', 4: 'B-ORG', 5: 'I-LOC', 6: 'I-MISC', 7: 'O'}\n"
     ]
    }
   ],
   "source": [
    "# print(f\"{word_to_ix = }\")\n",
    "print(f\"{ix_to_tag = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_model, word_index):\n",
    "    \"\"\"Create an embedding matrix given a word model (like Word2Vec) and a word_index (like the one from the Tokenizer)\"\"\"\n",
    "    vocab_size = len(word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "    embedding_dim = word_model.vector_size\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    \n",
    "    # Initialize UNK token to a random vector\n",
    "    unk_vector = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
    "    \n",
    "    # Initialize PAD token to zero vector\n",
    "    pad_vector = np.zeros((embedding_dim, ))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if word == \"UNK\":\n",
    "            embedding_matrix[i] = unk_vector\n",
    "        elif word == \"PAD\":\n",
    "            embedding_matrix[i] = pad_vector\n",
    "        else:\n",
    "            try:\n",
    "                # If word is in the model, use the corresponding embedding\n",
    "                embedding_vector = word_model[word]\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            except KeyError:\n",
    "                # If word is not in the model, use the UNK vector\n",
    "                embedding_matrix[i] = unk_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "pretrained_weights = create_embedding_matrix(word2vec_google, word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix, default=None):\n",
    "    \"\"\"Convert a sequence of words/tags to a sequence of indices.\"\"\"\n",
    "    if default:\n",
    "        return [to_ix.get(word, default) for word in seq]\n",
    "    return [to_ix[word] for word in seq]\n",
    "\n",
    "# Convert datasets into numerical sequences\n",
    "x_train = [prepare_sequence(sentence, word_to_ix, word_to_ix[\"UNK\"]) for sentence, _ in train_data]\n",
    "y_train = [prepare_sequence(tags, tag_to_ix) for _, tags in train_data]\n",
    "\n",
    "x_dev = [prepare_sequence(sentence, word_to_ix, word_to_ix[\"UNK\"]) for sentence, _ in dev_data]\n",
    "y_dev = [prepare_sequence(tags, tag_to_ix) for _, tags in dev_data]\n",
    "\n",
    "x_test = [prepare_sequence(sentence, word_to_ix, word_to_ix[\"UNK\"]) for sentence, _ in test_data]\n",
    "y_test = [prepare_sequence(tags, tag_to_ix) for _, tags in test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_INDEX = tag_to_ix['O']  # This should be 7\n",
    "MAX_LENGTH = max(len(sentence) for sentence, _ in train_data)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=MAX_LENGTH, padding='post', value=PAD_INDEX)\n",
    "y_train = pad_sequences(y_train, maxlen=MAX_LENGTH, padding='post', value=PAD_INDEX)\n",
    "\n",
    "x_dev = pad_sequences(x_dev, maxlen=MAX_LENGTH, padding='post', value=PAD_INDEX)\n",
    "y_dev = pad_sequences(y_dev, maxlen=MAX_LENGTH, padding='post', value=PAD_INDEX)\n",
    "\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_LENGTH, padding='post', value=PAD_INDEX)\n",
    "y_test = pad_sequences(y_test, maxlen=MAX_LENGTH, padding='post', value=PAD_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def NERLSTM(vocab_size, embedding_dim, hidden_dim, tagset_size, pretrained_weights):\n",
    "    # Input layer\n",
    "    sentence_input = Input(shape=(None, ), dtype='int32')\n",
    "    \n",
    "    # Load pre-trained embeddings\n",
    "    word_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[pretrained_weights], trainable=False)(sentence_input)\n",
    "    \n",
    "    # LSTM layer\n",
    "    lstm_out = LSTM(hidden_dim, return_sequences=True)(word_embeddings)\n",
    "    \n",
    "    # Dense output layer\n",
    "    tag_scores = TimeDistributed(Dense(tagset_size, activation='softmax'))(lstm_out)\n",
    "    \n",
    "    model = Model(sentence_input, tag_scores)\n",
    "    return model\n",
    "'''\n",
    "\n",
    "def NERLSTM(vocab_size, embedding_dim, hidden_dim, tagset_size, pretrained_weights):\n",
    "    # Input layer\n",
    "    sentence_input = Input(shape=(None,), dtype='int32')\n",
    "    \n",
    "    # Load pre-trained embeddings\n",
    "    word_embeddings = Embedding(input_dim=vocab_size, \n",
    "                                output_dim=embedding_dim, \n",
    "                                weights=[pretrained_weights],\n",
    "                                mask_zero=True,  # Since we are using padding\n",
    "                                trainable=False)(sentence_input)\n",
    "    \n",
    "    # LSTM layer with dropout\n",
    "    lstm_out = LSTM(hidden_dim,\n",
    "                    return_sequences=True,\n",
    "                    dropout=0.5)(word_embeddings)\n",
    "    \n",
    "    # Dense output layer with softmax\n",
    "    tag_scores = TimeDistributed(Dense(tagset_size, activation='softmax'))(lstm_out)\n",
    "    \n",
    "    model = Model(sentence_input, tag_scores)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Callback to implement\n",
    "#### \"Use the development set to evaluate the performance of the model for each epoch during training. Please use f1 score to measure the performance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "class F1Evaluation(Callback):\n",
    "    def __init__(self, validation_data=(), patience=3):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.x_val, self.y_val = validation_data\n",
    "        self.patience = patience\n",
    "        self.best_f1 = 0.0\n",
    "        self.wait = 0  # for early stopping\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.x_val, verbose=0)\n",
    "        y_pred = [list(np.argmax(pred, axis=-1)) for pred in y_pred]\n",
    "        y_true = [list(sentence) for sentence in self.y_val]\n",
    "        y_pred_str = [[ix_to_tag[ix] for ix in sentence] for sentence in y_pred]\n",
    "        y_true_str = [[ix_to_tag[ix] for ix in sentence] for sentence in y_true]\n",
    "\n",
    "        current_f1 = f1_score(y_true_str, y_pred_str)\n",
    "        logs['val_f1'] = current_f1\n",
    "\n",
    "        # Early Stopping if F1 Score is not increasing\n",
    "        if current_f1 > self.best_f1:\n",
    "            self.best_f1 = current_f1\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.model.stop_training = True\n",
    "                print(\"Early stopping based on F1 score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 10s 17ms/step - loss: 0.0393 - accuracy: 0.9883 - val_loss: 0.0326 - val_accuracy: 0.9920 - val_f1: 0.6430\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0213 - accuracy: 0.9938 - val_loss: 0.0300 - val_accuracy: 0.9927 - val_f1: 0.6873\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0184 - accuracy: 0.9947 - val_loss: 0.0289 - val_accuracy: 0.9929 - val_f1: 0.6927\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0168 - accuracy: 0.9951 - val_loss: 0.0287 - val_accuracy: 0.9931 - val_f1: 0.7058\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0156 - accuracy: 0.9955 - val_loss: 0.0270 - val_accuracy: 0.9935 - val_f1: 0.7293\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.0269 - val_accuracy: 0.9936 - val_f1: 0.7352\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0138 - accuracy: 0.9960 - val_loss: 0.0267 - val_accuracy: 0.9939 - val_f1: 0.7509\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0132 - accuracy: 0.9962 - val_loss: 0.0261 - val_accuracy: 0.9938 - val_f1: 0.7420\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0126 - accuracy: 0.9963 - val_loss: 0.0258 - val_accuracy: 0.9940 - val_f1: 0.7542\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0124 - accuracy: 0.9964 - val_loss: 0.0262 - val_accuracy: 0.9940 - val_f1: 0.7500\n",
      "Number of epochs: 10\n",
      "Running time: 66.7398669719696 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Set your model parameters\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 150\n",
    "VOCAB_SIZE = len(word_to_ix) + 1\n",
    "TAGSET_SIZE = len(tag_to_ix)\n",
    "\n",
    "# Initialize model\n",
    "model = NERLSTM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, TAGSET_SIZE, pretrained_weights)\n",
    "model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "f1_evaluation = F1Evaluation(validation_data=(x_dev, y_dev), patience=3)\n",
    "\n",
    "# Reshape y_train for the sparse_categorical_crossentropy loss\n",
    "y_train_reshaped = y_train.reshape(*y_train.shape, 1)\n",
    "y_dev_reshaped = y_dev.reshape(*y_dev.shape, 1)\n",
    "\n",
    "# Train the model and record the start and end time\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    x_train, y_train_reshaped, \n",
    "    epochs=10, \n",
    "    batch_size=32, \n",
    "    validation_data=(x_dev, y_dev_reshaped), \n",
    "    callbacks=[f1_evaluation],\n",
    "    shuffle=False  # Disable shuffling\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# (c) Report how many epochs and running time\n",
    "print(f\"Number of epochs: {len(history.epoch)}\")\n",
    "print(f\"Running time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 3ms/step\n",
      "F1 Score on the Test Set: 0.6421\n"
     ]
    }
   ],
   "source": [
    "# Convert the y_test from a numpy array to list of lists\n",
    "y_test_list = [list(sentence) for sentence in y_test]\n",
    "y_test_list_str = [[ix_to_tag[ix] for ix in sentence] for sentence in y_test_list]\n",
    "\n",
    "# Compute the F1 score on the test set\n",
    "test_predictions = model.predict(x_test)\n",
    "test_predictions = [list(np.argmax(pred, axis=-1)) for pred in test_predictions]\n",
    "test_predictions_str = [[ix_to_tag[ix] for ix in sentence] for sentence in test_predictions]\n",
    "test_f1 = f1_score(y_test_list_str, test_predictions_str)\n",
    "print(f\"F1 Score on the Test Set: {test_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
