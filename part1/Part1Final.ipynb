{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the \"word2vec-google-news-300\" embeddings\n",
    "glove_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: student\n",
      "Most similar word: students\n",
      "Cosine similarity: 0.7294867038726807\n",
      "--------\n",
      "Word: Apple\n",
      "Most similar word: Apple_AAPL\n",
      "Cosine similarity: 0.7456986308097839\n",
      "--------\n",
      "Word: apple\n",
      "Most similar word: apples\n",
      "Cosine similarity: 0.720359742641449\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "words = [\"student\", \"Apple\", \"apple\"]\n",
    "for word in words:\n",
    "    similar_word, similarity_score = glove_vectors.most_similar(word, topn=1)[0]\n",
    "    print(f\"Word: {word}\")\n",
    "    print(f\"Most similar word: {similar_word}\")\n",
    "    print(f\"Cosine similarity: {similarity_score}\")\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_file(file_path):\n",
    "    sentences = []  # This will store lists of word-label pairs, one list for each sentence\n",
    "    sentence = []   # Temporary list to store word-label pairs for the current sentence\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()  # Remove any leading/trailing whitespace\n",
    "            if line:  # If the line isn't empty (i.e., we're within a sentence)\n",
    "                word, _, _, label = line.split()  # Split the line to get the word and its associated label\n",
    "                sentence.append((word, label))\n",
    "            else:  # An empty line means the end of the current sentence\n",
    "                sentences.append(sentence)\n",
    "                sentence = []  # Reset the temporary list for the next sentence\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in training data: 14986\n",
      "Number of sentences in development data: 3465\n",
      "Number of sentences in test data: 3683\n",
      "All possible labels: {'I-ORG', 'B-LOC', 'I-PER', 'I-MISC', 'B-MISC', 'O', 'B-ORG', 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "train_data = read_conll_file(\"eng.train\")\n",
    "dev_data = read_conll_file(\"eng.testa\")\n",
    "test_data = read_conll_file(\"eng.testb\")\n",
    "\n",
    "print(f\"Number of sentences in training data: {len(train_data)}\")\n",
    "print(f\"Number of sentences in development data: {len(dev_data)}\")\n",
    "print(f\"Number of sentences in test data: {len(test_data)}\")\n",
    "\n",
    "# Extract all unique labels from training data\n",
    "labels = set()\n",
    "for sentence in train_data:\n",
    "    for _, label in sentence:\n",
    "        labels.add(label)\n",
    "\n",
    "print(f\"All possible labels: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Index with at least 2 named entities with more than one word: 5969\n",
      "Sentence: [['Swiss', 'NNP', 'I-NP', 'I-MISC'], ['Grand', 'NNP', 'I-NP', 'B-MISC'], ['Prix', 'NNP', 'I-NP', 'I-MISC'], ['World', 'NNP', 'I-NP', 'B-MISC'], ['Cup', 'NNP', 'I-NP', 'I-MISC'], ['cycling', 'NN', 'I-NP', 'O'], ['race', 'NN', 'I-NP', 'O'], ['on', 'IN', 'I-PP', 'O'], ['Sunday', 'NNP', 'I-NP', 'O'], [':', ':', 'O', 'O']]\n",
      "Raw Sentence: ['Swiss', 'Grand', 'Prix', 'World', 'Cup', 'cycling', 'race', 'on', 'Sunday', ':']\n",
      "Named Entities: ['Swiss', 'Grand Prix', 'World Cup']\n"
     ]
    }
   ],
   "source": [
    "# new part 1.2\n",
    "import codecs\n",
    "\n",
    "def load_sentences(path):\n",
    "    \"\"\"\n",
    "    Load sentences. A line must contain at least a word and its tag.\n",
    "    Sentences are separated by empty lines.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            sentence.append(word)\n",
    "\n",
    "    return sentences\n",
    "train_sentences = load_sentences('./eng.train')\n",
    "\n",
    "def list_named_entities(sentence):\n",
    "    named_entities = []\n",
    "    b_tag = False\n",
    "    for i in sentence:\n",
    "        if i[3].startswith('B'):\n",
    "            named_entities.append(i[0])\n",
    "            b_tag = True\n",
    "\n",
    "        elif i[3].startswith('I'):\n",
    "            if b_tag:\n",
    "                last_entity = named_entities[-1]\n",
    "                named_entities[-1] = last_entity + ' ' + i[0]\n",
    "                b_tag = False\n",
    "            else:\n",
    "                named_entities.append(i[0])\n",
    "            \n",
    "    return named_entities\n",
    "\n",
    "\n",
    "for index,i in enumerate(train_sentences):\n",
    "    numNamedEntities = 0\n",
    "    for j in i:\n",
    "      \n",
    "        if j[3].startswith('B'):\n",
    "            numNamedEntities+=1\n",
    "    if numNamedEntities>=2:\n",
    "        print(f'Sentence Index with at least 2 named entities with more than one word: {index}')\n",
    "        print(f'Sentence: {i}')\n",
    "        raw_sentence = [i[0] for i in i]\n",
    "        print(f'Raw Sentence: {raw_sentence}')\n",
    "        print(f'Named Entities: {list_named_entities(i)}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(file_name):\n",
    "    # Initialize empty lists to store sentences and NER tags\n",
    "    sentences = []\n",
    "    tags = []\n",
    "\n",
    "    # Read the content of the CoNLL2003 file\n",
    "    with open(file_name, 'r') as file:\n",
    "        current_sentence = []  # Initialize an empty list for the current sentence\n",
    "        current_tags = []  # Initialize an empty list for the current NER tags\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:  # Empty line indicates the end of a sentence\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    tags.append(current_tags)\n",
    "                current_sentence = []  # Reset the current sentence\n",
    "                current_tags = []  # Reset the current NER tags\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                current_sentence.append(parts[0])\n",
    "                current_tags.append(parts[-1])\n",
    "\n",
    "    # Add the last sentence if there's no empty line after it\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        tags.append(current_tags)\n",
    "\n",
    "    # Print the result\n",
    "    # for sentence, tag in zip(sentences, tags):\n",
    "    #     print(sentence)\n",
    "    #     print(tag)\n",
    "\n",
    "    #print(tags)\n",
    "    \n",
    "    return sentences, tags\n",
    "\n",
    "def extract_labels(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = file.read().strip()\n",
    "    \n",
    "    labels = set()\n",
    "    sentences = data.split('\\n\\n')\n",
    "    for sentence in sentences:\n",
    "        lines = sentence.split('\\n')\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) > 3:\n",
    "                label = parts[-1]\n",
    "                labels.add(label)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_labels = list(sorted(extract_labels(\"eng.train\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_tags = tokenize(\"eng.train\")\n",
    "dev_sentences, dev_tags = tokenize(\"eng.testa\")\n",
    "test_sentences, test_tags = tokenize(\"eng.testb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'text': train_sentences, 'labels': train_tags})\n",
    "df_dev = pd.DataFrame({'text': dev_sentences, 'labels': dev_tags})\n",
    "df_test = pd.DataFrame({'text': test_sentences, 'labels': test_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = glove_vectors.vectors\n",
    "zero_array = np.zeros((300,))\n",
    "embedding_matrix = np.vstack((embedding_matrix, zero_array)) # last element a zero array for OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_indices(tokens, model):\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            indices.append(model.get_index(token))\n",
    "        except:\n",
    "            indices.append(len(embedding_matrix)-1)  # Handle out-of-vocabulary words\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_indices(tokens):\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        indices.append(pos_train_labels.index(token))\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['word_indices'] = df_train['text'].apply(lambda x: tokens_to_indices(x, glove_vectors))\n",
    "df_dev['word_indices'] = df_dev['text'].apply(lambda x: tokens_to_indices(x, glove_vectors))\n",
    "df_test['word_indices'] = df_test['text'].apply(lambda x: tokens_to_indices(x, glove_vectors))\n",
    "\n",
    "df_train['label_indices'] = df_train['labels'].apply(lambda x: labels_to_indices(x))\n",
    "df_dev['label_indices'] = df_dev['labels'].apply(lambda x: labels_to_indices(x))\n",
    "df_test['label_indices'] = df_test['labels'].apply(lambda x: labels_to_indices(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final dataframe with indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>word_indices</th>\n",
       "      <th>label_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n",
       "      <td>[I-ORG, O, I-MISC, O, O, O, I-MISC, O, O]</td>\n",
       "      <td>[1611, 11500, 1760, 315, 3000000, 8059, 882, 1...</td>\n",
       "      <td>[5, 7, 4, 7, 7, 7, 4, 7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[I-PER, I-PER]</td>\n",
       "      <td>[1918, 9039]</td>\n",
       "      <td>[6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[I-LOC, O]</td>\n",
       "      <td>[24412, 3000000]</td>\n",
       "      <td>[3, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, European, Commission, said, on, Thursday...</td>\n",
       "      <td>[O, I-ORG, I-ORG, O, O, O, O, O, O, I-MISC, O,...</td>\n",
       "      <td>[7, 802, 1380, 9, 5, 224, 15, 10913, 8, 1760, ...</td>\n",
       "      <td>[7, 5, 5, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Germany, 's, representative, to, the, Europea...</td>\n",
       "      <td>[I-LOC, O, O, O, O, I-ORG, I-ORG, O, O, O, I-P...</td>\n",
       "      <td>[1420, 3000000, 3071, 3000000, 11, 802, 1745, ...</td>\n",
       "      <td>[3, 7, 7, 7, 7, 5, 5, 7, 7, 7, 6, 6, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14982</th>\n",
       "      <td>[Division, two]</td>\n",
       "      <td>[O, O]</td>\n",
       "      <td>[1747, 54]</td>\n",
       "      <td>[7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14983</th>\n",
       "      <td>[Plymouth, 2, Preston, 1]</td>\n",
       "      <td>[I-ORG, O, I-ORG, O]</td>\n",
       "      <td>[9487, 200, 8838, 165]</td>\n",
       "      <td>[5, 7, 5, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14984</th>\n",
       "      <td>[Division, three]</td>\n",
       "      <td>[O, O]</td>\n",
       "      <td>[1747, 80]</td>\n",
       "      <td>[7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14985</th>\n",
       "      <td>[Swansea, 1, Lincoln, 2]</td>\n",
       "      <td>[I-ORG, O, I-ORG, O]</td>\n",
       "      <td>[15741, 165, 3633, 200]</td>\n",
       "      <td>[5, 7, 5, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14986</th>\n",
       "      <td>[-DOCSTART-]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[3000000]</td>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14987 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      [EU, rejects, German, call, to, boycott, Briti...   \n",
       "1                                     [Peter, Blackburn]   \n",
       "2                                 [BRUSSELS, 1996-08-22]   \n",
       "3      [The, European, Commission, said, on, Thursday...   \n",
       "4      [Germany, 's, representative, to, the, Europea...   \n",
       "...                                                  ...   \n",
       "14982                                    [Division, two]   \n",
       "14983                          [Plymouth, 2, Preston, 1]   \n",
       "14984                                  [Division, three]   \n",
       "14985                           [Swansea, 1, Lincoln, 2]   \n",
       "14986                                       [-DOCSTART-]   \n",
       "\n",
       "                                                  labels  \\\n",
       "0              [I-ORG, O, I-MISC, O, O, O, I-MISC, O, O]   \n",
       "1                                         [I-PER, I-PER]   \n",
       "2                                             [I-LOC, O]   \n",
       "3      [O, I-ORG, I-ORG, O, O, O, O, O, O, I-MISC, O,...   \n",
       "4      [I-LOC, O, O, O, O, I-ORG, I-ORG, O, O, O, I-P...   \n",
       "...                                                  ...   \n",
       "14982                                             [O, O]   \n",
       "14983                               [I-ORG, O, I-ORG, O]   \n",
       "14984                                             [O, O]   \n",
       "14985                               [I-ORG, O, I-ORG, O]   \n",
       "14986                                                [O]   \n",
       "\n",
       "                                            word_indices  \\\n",
       "0      [1611, 11500, 1760, 315, 3000000, 8059, 882, 1...   \n",
       "1                                           [1918, 9039]   \n",
       "2                                       [24412, 3000000]   \n",
       "3      [7, 802, 1380, 9, 5, 224, 15, 10913, 8, 1760, ...   \n",
       "4      [1420, 3000000, 3071, 3000000, 11, 802, 1745, ...   \n",
       "...                                                  ...   \n",
       "14982                                         [1747, 54]   \n",
       "14983                             [9487, 200, 8838, 165]   \n",
       "14984                                         [1747, 80]   \n",
       "14985                            [15741, 165, 3633, 200]   \n",
       "14986                                          [3000000]   \n",
       "\n",
       "                                           label_indices  \n",
       "0                            [5, 7, 4, 7, 7, 7, 4, 7, 7]  \n",
       "1                                                 [6, 6]  \n",
       "2                                                 [3, 7]  \n",
       "3      [7, 5, 5, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, ...  \n",
       "4      [3, 7, 7, 7, 7, 5, 5, 7, 7, 7, 6, 6, 7, 7, 7, ...  \n",
       "...                                                  ...  \n",
       "14982                                             [7, 7]  \n",
       "14983                                       [5, 7, 5, 7]  \n",
       "14984                                             [7, 7]  \n",
       "14985                                       [5, 7, 5, 7]  \n",
       "14986                                                [7]  \n",
       "\n",
       "[14987 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>word_indices</th>\n",
       "      <th>label_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
       "      <td>[O, O, I-LOC, O, O, O, O, I-PER, O, O, O, O]</td>\n",
       "      <td>[62630, 3000000, 75008, 47508, 156520, 31952, ...</td>\n",
       "      <td>[7, 7, 3, 7, 7, 7, 7, 6, 7, 7, 7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>[I-PER, I-PER]</td>\n",
       "      <td>[166344, 772342]</td>\n",
       "      <td>[6, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
       "      <td>[I-LOC, O, I-LOC, I-LOC, I-LOC, O]</td>\n",
       "      <td>[3000000, 3000000, 1217, 2658, 11668, 3000000]</td>\n",
       "      <td>[3, 7, 3, 3, 3, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
       "      <td>[I-LOC, O, O, O, O, O, I-MISC, I-MISC, O, O, O...</td>\n",
       "      <td>[922, 548, 11, 3000000, 3000000, 30, 2125, 209...</td>\n",
       "      <td>[3, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
       "      <td>[O, I-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[84, 367, 653, 30, 4446, 6796, 82, 1, 11, 110,...</td>\n",
       "      <td>[7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679</th>\n",
       "      <td>[\", It, was, the, joy, that, we, all, had, ove...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[3000000, 51, 10, 11, 5942, 3, 38, 52, 35, 63,...</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3680</th>\n",
       "      <td>[Charlton, managed, Ireland, for, 93, matches,...</td>\n",
       "      <td>[I-PER, O, I-LOC, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[13572, 1464, 2620, 2, 3000000, 2004, 3000000,...</td>\n",
       "      <td>[6, 7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>[He, guided, Ireland, to, two, successive, Wor...</td>\n",
       "      <td>[O, O, I-LOC, O, O, O, I-MISC, I-MISC, O, O, O...</td>\n",
       "      <td>[57, 7064, 2620, 3000000, 54, 8339, 796, 2094,...</td>\n",
       "      <td>[7, 7, 3, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3682</th>\n",
       "      <td>[The, lanky, former, Leeds, United, defender, ...</td>\n",
       "      <td>[O, O, O, I-ORG, I-ORG, O, O, O, O, O, I-LOC, ...</td>\n",
       "      <td>[7, 33489, 249, 7178, 1217, 4206, 92, 13, 109,...</td>\n",
       "      <td>[7, 7, 7, 5, 5, 7, 7, 7, 7, 7, 3, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3683</th>\n",
       "      <td>[-DOCSTART-]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[3000000]</td>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3684 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
       "1                                        [Nadim, Ladki]   \n",
       "2       [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
       "3     [Japan, began, the, defence, of, their, Asian,...   \n",
       "4     [But, China, saw, their, luck, desert, them, i...   \n",
       "...                                                 ...   \n",
       "3679  [\", It, was, the, joy, that, we, all, had, ove...   \n",
       "3680  [Charlton, managed, Ireland, for, 93, matches,...   \n",
       "3681  [He, guided, Ireland, to, two, successive, Wor...   \n",
       "3682  [The, lanky, former, Leeds, United, defender, ...   \n",
       "3683                                       [-DOCSTART-]   \n",
       "\n",
       "                                                 labels  \\\n",
       "0          [O, O, I-LOC, O, O, O, O, I-PER, O, O, O, O]   \n",
       "1                                        [I-PER, I-PER]   \n",
       "2                    [I-LOC, O, I-LOC, I-LOC, I-LOC, O]   \n",
       "3     [I-LOC, O, O, O, O, O, I-MISC, I-MISC, O, O, O...   \n",
       "4     [O, I-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "...                                                 ...   \n",
       "3679  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3680  [I-PER, O, I-LOC, O, O, O, O, O, O, O, O, O, O...   \n",
       "3681  [O, O, I-LOC, O, O, O, I-MISC, I-MISC, O, O, O...   \n",
       "3682  [O, O, O, I-ORG, I-ORG, O, O, O, O, O, I-LOC, ...   \n",
       "3683                                                [O]   \n",
       "\n",
       "                                           word_indices  \\\n",
       "0     [62630, 3000000, 75008, 47508, 156520, 31952, ...   \n",
       "1                                      [166344, 772342]   \n",
       "2        [3000000, 3000000, 1217, 2658, 11668, 3000000]   \n",
       "3     [922, 548, 11, 3000000, 3000000, 30, 2125, 209...   \n",
       "4     [84, 367, 653, 30, 4446, 6796, 82, 1, 11, 110,...   \n",
       "...                                                 ...   \n",
       "3679  [3000000, 51, 10, 11, 5942, 3, 38, 52, 35, 63,...   \n",
       "3680  [13572, 1464, 2620, 2, 3000000, 2004, 3000000,...   \n",
       "3681  [57, 7064, 2620, 3000000, 54, 8339, 796, 2094,...   \n",
       "3682  [7, 33489, 249, 7178, 1217, 4206, 92, 13, 109,...   \n",
       "3683                                          [3000000]   \n",
       "\n",
       "                                          label_indices  \n",
       "0                  [7, 7, 3, 7, 7, 7, 7, 6, 7, 7, 7, 7]  \n",
       "1                                                [6, 6]  \n",
       "2                                    [3, 7, 3, 3, 3, 7]  \n",
       "3     [3, 7, 7, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "4     [7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "...                                                 ...  \n",
       "3679  [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "3680  [6, 7, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "3681  [7, 7, 3, 7, 7, 7, 4, 4, 7, 7, 7, 7, 7, 7, 4, ...  \n",
       "3682  [7, 7, 7, 5, 5, 7, 7, 7, 7, 7, 3, 7, 7, 7, 7, ...  \n",
       "3683                                                [7]  \n",
       "\n",
       "[3684 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>word_indices</th>\n",
       "      <th>label_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CRICKET, -, LEICESTERSHIRE, TAKE, OVER, AT, T...</td>\n",
       "      <td>[O, O, I-ORG, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[93620, 3000000, 1552803, 71662, 36028, 11030,...</td>\n",
       "      <td>[7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[LONDON, 1996-08-30]</td>\n",
       "      <td>[I-LOC, O]</td>\n",
       "      <td>[4949, 3000000]</td>\n",
       "      <td>[3, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[West, Indian, all-rounder, Phil, Simmons, too...</td>\n",
       "      <td>[I-MISC, I-MISC, O, I-PER, I-PER, O, O, O, O, ...</td>\n",
       "      <td>[611, 1106, 3000000, 4811, 8262, 263, 134, 2, ...</td>\n",
       "      <td>[4, 4, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 5, 7, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Their, stay, on, top, ,, though, ,, may, be, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, I-ORG,...</td>\n",
       "      <td>[1697, 820, 5, 213, 3000000, 459, 3000000, 137...</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 5, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[After, bowling, Somerset, out, for, 83, on, t...</td>\n",
       "      <td>[O, O, I-ORG, O, O, O, O, O, O, O, O, I-LOC, I...</td>\n",
       "      <td>[361, 6842, 11008, 49, 2, 3000000, 5, 11, 798,...</td>\n",
       "      <td>[7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 7, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>[Brokers, said, blue, chips, like, IDLC, ,, Ba...</td>\n",
       "      <td>[O, O, O, O, O, I-ORG, O, I-ORG, I-ORG, O, I-O...</td>\n",
       "      <td>[29281, 9, 2836, 5695, 87, 1325185, 3000000, 5...</td>\n",
       "      <td>[7, 7, 7, 7, 7, 5, 7, 5, 5, 7, 5, 5, 7, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>[They, said, there, was, still, demand, for, b...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[128, 9, 72, 10, 151, 687, 2, 2836, 5695, 1, 2...</td>\n",
       "      <td>[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>[The, DSE, all, share, price, index, closed, 2...</td>\n",
       "      <td>[O, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[7, 78228, 52, 306, 422, 1542, 866, 3000000, 1...</td>\n",
       "      <td>[7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>[--, Dhaka, Newsroom, 880-2-506363]</td>\n",
       "      <td>[O, I-ORG, I-ORG, O]</td>\n",
       "      <td>[3000000, 15087, 53872, 3000000]</td>\n",
       "      <td>[7, 5, 5, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>[-DOCSTART-]</td>\n",
       "      <td>[O]</td>\n",
       "      <td>[3000000]</td>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3466 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     [CRICKET, -, LEICESTERSHIRE, TAKE, OVER, AT, T...   \n",
       "1                                  [LONDON, 1996-08-30]   \n",
       "2     [West, Indian, all-rounder, Phil, Simmons, too...   \n",
       "3     [Their, stay, on, top, ,, though, ,, may, be, ...   \n",
       "4     [After, bowling, Somerset, out, for, 83, on, t...   \n",
       "...                                                 ...   \n",
       "3461  [Brokers, said, blue, chips, like, IDLC, ,, Ba...   \n",
       "3462  [They, said, there, was, still, demand, for, b...   \n",
       "3463  [The, DSE, all, share, price, index, closed, 2...   \n",
       "3464                [--, Dhaka, Newsroom, 880-2-506363]   \n",
       "3465                                       [-DOCSTART-]   \n",
       "\n",
       "                                                 labels  \\\n",
       "0                 [O, O, I-ORG, O, O, O, O, O, O, O, O]   \n",
       "1                                            [I-LOC, O]   \n",
       "2     [I-MISC, I-MISC, O, I-PER, I-PER, O, O, O, O, ...   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, I-ORG,...   \n",
       "4     [O, O, I-ORG, O, O, O, O, O, O, O, O, I-LOC, I...   \n",
       "...                                                 ...   \n",
       "3461  [O, O, O, O, O, I-ORG, O, I-ORG, I-ORG, O, I-O...   \n",
       "3462  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3463  [O, I-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "3464                               [O, I-ORG, I-ORG, O]   \n",
       "3465                                                [O]   \n",
       "\n",
       "                                           word_indices  \\\n",
       "0     [93620, 3000000, 1552803, 71662, 36028, 11030,...   \n",
       "1                                       [4949, 3000000]   \n",
       "2     [611, 1106, 3000000, 4811, 8262, 263, 134, 2, ...   \n",
       "3     [1697, 820, 5, 213, 3000000, 459, 3000000, 137...   \n",
       "4     [361, 6842, 11008, 49, 2, 3000000, 5, 11, 798,...   \n",
       "...                                                 ...   \n",
       "3461  [29281, 9, 2836, 5695, 87, 1325185, 3000000, 5...   \n",
       "3462  [128, 9, 72, 10, 151, 687, 2, 2836, 5695, 1, 2...   \n",
       "3463  [7, 78228, 52, 306, 422, 1542, 866, 3000000, 1...   \n",
       "3464                   [3000000, 15087, 53872, 3000000]   \n",
       "3465                                          [3000000]   \n",
       "\n",
       "                                          label_indices  \n",
       "0                     [7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7]  \n",
       "1                                                [3, 7]  \n",
       "2     [4, 4, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 5, 7, 5, ...  \n",
       "3     [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 5, 7, ...  \n",
       "4     [7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 7, 5, ...  \n",
       "...                                                 ...  \n",
       "3461  [7, 7, 7, 7, 7, 5, 7, 5, 5, 7, 5, 5, 7, 5, 5, ...  \n",
       "3462  [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "3463  [7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ...  \n",
       "3464                                       [7, 5, 5, 7]  \n",
       "3465                                                [7]  \n",
       "\n",
       "[3466 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create torch datasets and dataloader from DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data, targets = zip(*batch)\n",
    "    \n",
    "    # Sort the batch by sequence length (optional, but can improve efficiency)\n",
    "    sorted_indices = sorted(range(len(data)), key=lambda i: len(data[i]), reverse=True)\n",
    "    data = [data[i] for i in sorted_indices]\n",
    "    targets = [torch.tensor(targets[i]) for i in sorted_indices]\n",
    "\n",
    "    # Create a list of sequences and their corresponding lengths\n",
    "    sequences = [torch.tensor(seq) for seq in data]\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "    # Pad the sequences to the length of the longest sequence in the batch\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=7)\n",
    "\n",
    "    # Create packed sequence for RNNs (optional, if you're using an RNN)\n",
    "    # packed_sequences = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    return padded_sequences, padded_targets, torch.tensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyCustomDataset(df_train['word_indices'].to_numpy(), df_train['label_indices'].to_numpy())\n",
    "test_dataset = MyCustomDataset(df_test['word_indices'].to_numpy(), df_test['label_indices'].to_numpy())\n",
    "dev_dataset = MyCustomDataset(df_dev['word_indices'].to_numpy(), df_dev['label_indices'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "development_dataloader = DataLoader(dev_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, bidirectional, device):\n",
    "        super(LSTMTextClassifier, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.device = device\n",
    "        # Embedding layer with pretrained word vectors\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix).to(device))\n",
    "        self.embedding.weight.requires_grad = False # freeze the embeddings\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        \n",
    "        # # Softmax Layer\n",
    "        self.softmax_layer = nn.Linear(hidden_dim, self.output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text.to(torch.int64))\n",
    "        \n",
    "        #Pack the embedded sequences to handle variable-length sequences\n",
    "        packed_embedded = pack_padded_sequence(embedded, text_lengths.cpu().to(torch.int64), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Pass through the LSTM layer\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "\n",
    "        # Unpack the packed sequences\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        # Pass through the softmax layer\n",
    "        output = self.softmax_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "hidden_dim = 256\n",
    "output_dim = 8\n",
    "vocab_size = len(glove_vectors.index_to_key)\n",
    "embedding_dim = 300\n",
    "num_layer = 1\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, valid_dataloader, optimizer, loss_fn, device, num_epochs=50):\n",
    "    model.to(device)\n",
    "    dev_f1_per_epoch = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        # ======== training phase ==========\n",
    "        train_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            text, labels, text_lengths = batch\n",
    "            text, labels, text_lengths  = torch.tensor(text).to(device), labels.to(device), text_lengths\n",
    "\n",
    "            optimizer.zero_grad() # clear gradients\n",
    "\n",
    "            output = model(text, text_lengths)\n",
    "            \n",
    "            output = output.view(-1, 8)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        # ====================================\n",
    "\n",
    "\n",
    "        # ========== validation phase =========\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        f1 = 0\n",
    "\n",
    "        predictions = []\n",
    "        label_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_dataloader:\n",
    "                text, labels, text_lengths = batch\n",
    "                text, labels, text_lengths  = text.to(device), labels.to(device), text_lengths\n",
    "\n",
    "                output = model(text, text_lengths)\n",
    "                bs, sl, inp = output.size()\n",
    "                output = output.view(-1, 8)\n",
    "                labels = labels.view(-1)\n",
    "                loss = loss_fn(output, labels)\n",
    "                output = output.view(bs, sl, inp)\n",
    "                labels = labels.view(bs, sl)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output, 2)\n",
    "                for i in range(predicted.size(0)):\n",
    "                    pred = []\n",
    "                    lbl = []\n",
    "                    for j in predicted[i]:\n",
    "                        pred.append(pos_train_labels[j])\n",
    "                    for k in labels[i]:\n",
    "                        lbl.append(pos_train_labels[k])\n",
    "                    predictions.append(pred)\n",
    "                    label_list.append(lbl)\n",
    "\n",
    "                # print(\"output:\", output.shape)\n",
    "                # print(\"predicted:\", predicted.shape)\n",
    "                # print(\"labels:\", labels.shape)\n",
    "                # f1 = f1 + f1_score(labels, predicted)\n",
    "                # total += labels.size(0)\n",
    "                # correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "        # Calculate average validation loss and accuracy for the epoch\n",
    "        avg_valid_loss = valid_loss / len(valid_dataloader)\n",
    "        # print(\"label list:\", label_list)\n",
    "        # print(\"predictions:\",predictions)\n",
    "        f1 = f1_score(label_list, predictions)\n",
    "        dev_f1_per_epoch.append(f1)\n",
    "        # =======================================\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_valid_loss:.4f}, F1: {f1:.2f}')\n",
    "\n",
    "    print('Training complete.')\n",
    "\n",
    "    return dev_f1_per_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataloader, device, loss_fn):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    label_list = []\n",
    "    with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                text, labels, text_lengths = batch\n",
    "                text, labels, text_lengths  = text.to(device), labels.to(device), text_lengths\n",
    "\n",
    "                output = model(text, text_lengths)\n",
    "                bs, sl, inp = output.size()\n",
    "                output = output.view(-1, 8)\n",
    "                labels = labels.view(-1)\n",
    "                loss = loss_fn(output, labels)\n",
    "                output = output.view(bs, sl, inp)\n",
    "                labels = labels.view(bs, sl)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output, 2)\n",
    "                for i in range(predicted.size(0)):\n",
    "                    pred = []\n",
    "                    lbl = []\n",
    "                    for j in predicted[i]:\n",
    "                        pred.append(pos_train_labels[j])\n",
    "                    for k in labels[i]:\n",
    "                        lbl.append(pos_train_labels[k])\n",
    "                    predictions.append(pred)\n",
    "                    label_list.append(lbl)\n",
    "\n",
    "                # print(\"output:\", output.shape)\n",
    "                # print(\"predicted:\", predicted.shape)\n",
    "                # print(\"labels:\", labels.shape)\n",
    "                # f1 = f1 + f1_score(labels, predicted)\n",
    "                # total += labels.size(0)\n",
    "                # correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    # Calculate average validation loss and accuracy for the epoch\n",
    "    avg_valid_loss = valid_loss / len(test_dataloader)\n",
    "    # print(\"label list:\", label_list)\n",
    "    # print(\"predictions:\",predictions)\n",
    "    f1 = f1_score(label_list, predictions)\n",
    "\n",
    "    return avg_valid_loss, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Timothy\\AppData\\Local\\Temp\\ipykernel_7316\\4238509672.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix).to(device))\n"
     ]
    }
   ],
   "source": [
    "lstm_classifier = LSTMTextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim,num_layer,False, device).to(device)\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(lstm_classifier.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Timothy\\AppData\\Local\\Temp\\ipykernel_7316\\576090230.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  text, labels, text_lengths  = torch.tensor(text).to(device), labels.to(device), text_lengths\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 1.2068, Validation Loss: 0.8987, F1: 0.71\n",
      "Epoch 2/15, Train Loss: 0.7339, Validation Loss: 0.5707, F1: 0.76\n",
      "Epoch 3/15, Train Loss: 0.4661, Validation Loss: 0.3655, F1: 0.78\n",
      "Epoch 4/15, Train Loss: 0.3035, Validation Loss: 0.2450, F1: 0.79\n",
      "Epoch 5/15, Train Loss: 0.2074, Validation Loss: 0.1743, F1: 0.80\n",
      "Epoch 6/15, Train Loss: 0.1489, Validation Loss: 0.1313, F1: 0.81\n",
      "Epoch 7/15, Train Loss: 0.1117, Validation Loss: 0.1038, F1: 0.81\n",
      "Epoch 8/15, Train Loss: 0.0866, Validation Loss: 0.0856, F1: 0.82\n",
      "Epoch 9/15, Train Loss: 0.0689, Validation Loss: 0.0734, F1: 0.82\n",
      "Epoch 10/15, Train Loss: 0.0558, Validation Loss: 0.0654, F1: 0.82\n",
      "Epoch 11/15, Train Loss: 0.0457, Validation Loss: 0.0601, F1: 0.82\n",
      "Epoch 12/15, Train Loss: 0.0378, Validation Loss: 0.0574, F1: 0.81\n",
      "Epoch 13/15, Train Loss: 0.0316, Validation Loss: 0.0554, F1: 0.81\n",
      "Epoch 14/15, Train Loss: 0.0267, Validation Loss: 0.0536, F1: 0.82\n",
      "Epoch 15/15, Train Loss: 0.0226, Validation Loss: 0.0538, F1: 0.81\n",
      "Training complete.\n",
      "Elapsed time: 398.0248193740845 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "dev_acc_per_epoch = train(lstm_classifier, train_dataloader, development_dataloader, optimizer, loss_fn, device, num_epochs=15)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 0.086020651293501\n",
      "F1 Score: 0.7582141309059385\n"
     ]
    }
   ],
   "source": [
    "avg_valid_loss, f1 = test(lstm_classifier, test_dataloader, device, loss_fn)\n",
    "\n",
    "print(\"Avg test loss:\", avg_valid_loss)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
