{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import string\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(word2vec.index_to_key)\n",
    "vocab_size = len(word2vec.index_to_key)\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('dataset part 2/train.csv')\n",
    "test_df = pd.read_csv(\"dataset part 2/test.csv\")\n",
    "train_df.drop(columns=['label-fine'], inplace=True)\n",
    "test_df.drop(columns=['label-fine'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>1</td>\n",
       "      <td>What 's the shape of a camel 's spine ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>1</td>\n",
       "      <td>What type of currency is used in China ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>4</td>\n",
       "      <td>What is the temperature today ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>4</td>\n",
       "      <td>What is the temperature for cooking ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451</th>\n",
       "      <td>1</td>\n",
       "      <td>What currency is used in Australia ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5452 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label-coarse                                               text\n",
       "0                0  How did serfdom develop in and then leave Russ...\n",
       "1                1   What films featured the character Popeye Doyle ?\n",
       "2                0  How can I find a list of celebrities ' real na...\n",
       "3                1  What fowl grabs the spotlight after the Chines...\n",
       "4                2                    What is the full form of .com ?\n",
       "...            ...                                                ...\n",
       "5447             1            What 's the shape of a camel 's spine ?\n",
       "5448             1           What type of currency is used in China ?\n",
       "5449             4                    What is the temperature today ?\n",
       "5450             4              What is the temperature for cooking ?\n",
       "5451             1               What currency is used in Australia ?\n",
       "\n",
       "[5452 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form a development set from a random subset (containing 500 examples) within the original training data. Remove these examples from original training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_df = train_df.sample(n=500, random_state=0) # randomly sample 500 rows from train df\n",
    "train_df = train_df.drop(development_df.index) # remove sampled rows from train df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>4</td>\n",
       "      <td>How many trees go into paper making in a year ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>1</td>\n",
       "      <td>What concerts are held in New York this week ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>5</td>\n",
       "      <td>Where did the sport of caber-tossing originate ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>3</td>\n",
       "      <td>What kind of people took part in Shays ' Rebel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2812</th>\n",
       "      <td>0</td>\n",
       "      <td>How is cologne made ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5335</th>\n",
       "      <td>5</td>\n",
       "      <td>Where was George Washington born ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>4</td>\n",
       "      <td>In what year did the Bounty mutiny happen ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4508</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the difference between khaki and chino ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>4</td>\n",
       "      <td>How many types of dogs ' tails are there - three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>0</td>\n",
       "      <td>What is a `` node '' in computer terms ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label-coarse                                               text\n",
       "2755             4    How many trees go into paper making in a year ?\n",
       "3326             1     What concerts are held in New York this week ?\n",
       "2204             5   Where did the sport of caber-tossing originate ?\n",
       "2888             3  What kind of people took part in Shays ' Rebel...\n",
       "2812             0                              How is cologne made ?\n",
       "...            ...                                                ...\n",
       "5335             5                 Where was George Washington born ?\n",
       "1275             4        In what year did the Bounty mutiny happen ?\n",
       "4508             0   What is the difference between khaki and chino ?\n",
       "402              4   How many types of dogs ' tails are there - three\n",
       "4386             0           What is a `` node '' in computer terms ?\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "development_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select 4 classes from 6 coarse labels, combine remaining 2 to form a single class 'OTHERS'. Adjust original data such that label for each sentence is updated accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected 2 labels for combining\n",
    "combined_1 = 4\n",
    "combined_2 = 5\n",
    "\n",
    "# train_df['label-coarse'] = train_df['label-coarse'].astype(object)\n",
    "# test_df['label-coarse'] = train_df['label-coarse'].astype(object)\n",
    "# development_df['label-coarse'] = train_df['label-coarse'].astype(object)\n",
    "\n",
    "# Assign label 5 as 4 as the OTHERS label\n",
    "train_df.loc[(train_df['label-coarse'] == combined_2), 'label-coarse'] = combined_1\n",
    "test_df.loc[(test_df['label-coarse'] == combined_2), 'label-coarse'] = combined_1\n",
    "development_df.loc[(development_df['label-coarse'] == combined_2), 'label-coarse'] = combined_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_and_remove_punctuation(text):\n",
    "    # Create a translation table to replace punctuation with empty strings\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # Remove punctuation and replace with ''\n",
    "    text_ = text.translate(translator)\n",
    "    tokens = word_tokenize(text_)\n",
    "    # filtered_tokens = [token for token in tokens if token in vocab]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].apply(lambda x: tokenize_and_remove_punctuation(x))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: tokenize_and_remove_punctuation(x))\n",
    "development_df['text'] = development_df['text'].apply(lambda x: tokenize_and_remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding of Data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pad_tokens(tokens, max_length, padding_value='<pad>'):\n",
    "#     if len(tokens) < max_length:\n",
    "#         tokens.extend([padding_value] * (max_length - len(tokens)))\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = max(train_df['text'].apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['text'] = train_df['text'].apply(pad_tokens, max_length=max_length)\n",
    "# test_df['text'] = test_df['text'].apply(pad_tokens, max_length=max_length)\n",
    "# development_df['text'] = development_df['text'].apply(pad_tokens, max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embedding matrix for embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = word2vec.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last element a zero array for out of vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_array = np.zeros((300,))\n",
    "embedding_matrix = np.vstack((embedding_matrix, zero_array)) # last element a zero array for OOV words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode text in dataframe to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_indices(tokens, model):\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            indices.append(model.get_index(token))\n",
    "        except:\n",
    "            indices.append(len(embedding_matrix)-1)  # Handle out-of-vocabulary words\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['word_indices'] = train_df['text'].apply(lambda x: tokens_to_indices(x, word2vec))\n",
    "test_df['word_indices'] = test_df['text'].apply(lambda x: tokens_to_indices(x, word2vec))\n",
    "development_df['word_indices'] = development_df['text'].apply(lambda x: tokens_to_indices(x, word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>text</th>\n",
       "      <th>word_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[How, did, serfdom, develop, in, and, then, le...</td>\n",
       "      <td>[1190, 92, 178836, 1306, 1, 3000000, 145, 785,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[What, films, featured, the, character, Popeye...</td>\n",
       "      <td>[470, 2485, 2723, 11, 1980, 65760, 8347]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[How, can, I, find, a, list, of, celebrities, ...</td>\n",
       "      <td>[1190, 50, 20, 359, 3000000, 711, 3000000, 666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[What, fowl, grabs, the, spotlight, after, the...</td>\n",
       "      <td>[470, 43119, 10673, 11, 6236, 55, 11, 1035, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[What, is, the, full, form, of, com]</td>\n",
       "      <td>[470, 4, 11, 335, 815, 3000000, 18117]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>1</td>\n",
       "      <td>[What, s, the, shape, of, a, camel, s, spine]</td>\n",
       "      <td>[470, 1280, 11, 2790, 3000000, 3000000, 28380,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>1</td>\n",
       "      <td>[What, type, of, currency, is, used, in, China]</td>\n",
       "      <td>[470, 1474, 3000000, 2325, 4, 233, 1, 367]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>4</td>\n",
       "      <td>[What, is, the, temperature, today]</td>\n",
       "      <td>[470, 4, 11, 4360, 205]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>4</td>\n",
       "      <td>[What, is, the, temperature, for, cooking]</td>\n",
       "      <td>[470, 4, 11, 4360, 2, 5195]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451</th>\n",
       "      <td>1</td>\n",
       "      <td>[What, currency, is, used, in, Australia]</td>\n",
       "      <td>[470, 2325, 4, 233, 1, 904]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4952 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label-coarse                                               text  \\\n",
       "0                0  [How, did, serfdom, develop, in, and, then, le...   \n",
       "1                1  [What, films, featured, the, character, Popeye...   \n",
       "2                0  [How, can, I, find, a, list, of, celebrities, ...   \n",
       "3                1  [What, fowl, grabs, the, spotlight, after, the...   \n",
       "4                2               [What, is, the, full, form, of, com]   \n",
       "...            ...                                                ...   \n",
       "5447             1      [What, s, the, shape, of, a, camel, s, spine]   \n",
       "5448             1    [What, type, of, currency, is, used, in, China]   \n",
       "5449             4                [What, is, the, temperature, today]   \n",
       "5450             4         [What, is, the, temperature, for, cooking]   \n",
       "5451             1          [What, currency, is, used, in, Australia]   \n",
       "\n",
       "                                           word_indices  \n",
       "0     [1190, 92, 178836, 1306, 1, 3000000, 145, 785,...  \n",
       "1              [470, 2485, 2723, 11, 1980, 65760, 8347]  \n",
       "2     [1190, 50, 20, 359, 3000000, 711, 3000000, 666...  \n",
       "3     [470, 43119, 10673, 11, 6236, 55, 11, 1035, 16...  \n",
       "4                [470, 4, 11, 335, 815, 3000000, 18117]  \n",
       "...                                                 ...  \n",
       "5447  [470, 1280, 11, 2790, 3000000, 3000000, 28380,...  \n",
       "5448         [470, 1474, 3000000, 2325, 4, 233, 1, 367]  \n",
       "5449                            [470, 4, 11, 4360, 205]  \n",
       "5450                        [470, 4, 11, 4360, 2, 5195]  \n",
       "5451                        [470, 2325, 4, 233, 1, 904]  \n",
       "\n",
       "[4952 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom dataset, collate function for dataloader and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data, targets = zip(*batch)\n",
    "    \n",
    "    # Sort the batch by sequence length (optional, but can improve efficiency)\n",
    "    sorted_indices = sorted(range(len(data)), key=lambda i: len(data[i]), reverse=True)\n",
    "    data = [data[i] for i in sorted_indices]\n",
    "    targets = [targets[i] for i in sorted_indices]\n",
    "\n",
    "    # Create a list of sequences and their corresponding lengths\n",
    "    sequences = [torch.tensor(seq) for seq in data]\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "    # Pad the sequences to the length of the longest sequence in the batch\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    # Create packed sequence for RNNs (optional, if you're using an RNN)\n",
    "    # packed_sequences = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    return padded_sequences, torch.tensor(targets), torch.tensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyCustomDataset(train_df['word_indices'].to_numpy(), train_df['label-coarse'].to_numpy())\n",
    "test_dataset = MyCustomDataset(test_df['word_indices'].to_numpy(), test_df['label-coarse'].to_numpy())\n",
    "development_dataset = MyCustomDataset(development_df['word_indices'].to_numpy(), development_df['label-coarse'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "development_dataloader = DataLoader(development_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Text Classifier Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, bidirectional, device, aggregate='last'):\n",
    "        super(LSTMTextClassifier, self).__init__()\n",
    "        self.aggregate = aggregate\n",
    "        # Embedding layer with pretrained word vectors\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix).to(device))\n",
    "        self.embedding.weight.requires_grad = False # freeze the embeddings\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        \n",
    "        # Softmax Layer\n",
    "        self.softmax_layer = nn.Sequential(nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim),\n",
    "        nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        # Embed the input\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #Pack the embedded sequences to handle variable-length sequences\n",
    "        packed_embedded = pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Pass through the LSTM layer\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "\n",
    "        # Unpack the packed sequences\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        if(self.aggregate == 'last'):\n",
    "            # Use the final hidden state as the representation for the sentence\n",
    "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1) if self.lstm.bidirectional else hidden[-1, :, :]\n",
    "            # Use max pooling to aggregate as the representation for the sentence\n",
    "        elif(self.aggregate == 'max'):\n",
    "            hidden, _ = torch.max(output, dim = 1)\n",
    "            # Use average pooling to aggregate as the representation for the sentence\n",
    "        elif(self.aggregate == 'average'):\n",
    "            hidden = torch.mean(output, dim = 1)\n",
    "        \n",
    "        # Pass through the softmax layer\n",
    "        output = self.softmax_layer(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "hidden_dim = 256\n",
    "output_dim = 5\n",
    "vocab_size = len(word2vec.index_to_key)\n",
    "embedding_dim = 300\n",
    "num_layer = 1\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, valid_dataloader, optimizer, loss_fn, device, num_epochs=50):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        # ======== training phase ==========\n",
    "        train_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            text, labels, text_lengths = batch\n",
    "            text, labels, text_lengths  = torch.tensor(text).to(device), labels.to(device), text_lengths\n",
    "\n",
    "            optimizer.zero_grad() # clear gradients\n",
    "\n",
    "            output = model(text, text_lengths)\n",
    "\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss for the epoch\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        # ====================================\n",
    "\n",
    "\n",
    "        # ========== validation phase =========\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_dataloader:\n",
    "                text, labels, text_lengths = batch\n",
    "                text, labels, text_lengths  = text.to(device), labels.to(device), text_lengths\n",
    "\n",
    "                output = model(text, text_lengths)\n",
    "                loss = loss_fn(output, labels)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average validation loss and accuracy for the epoch\n",
    "        avg_valid_loss = valid_loss / len(valid_dataloader)\n",
    "        accuracy = 100 * (correct / total)\n",
    "        # =======================================\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_valid_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    print('Training complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataloader, device, loss_fn):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            text, labels, text_lengths = batch\n",
    "            text, labels, text_lengths  = text.to(device), labels.to(device), text_lengths\n",
    "\n",
    "            output = model(text, text_lengths)\n",
    "            loss = loss_fn(output, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average validation loss and accuracy for the epoch\n",
    "    avg_valid_loss = valid_loss / len(test_dataloader)\n",
    "    accuracy = 100 * (correct / total)\n",
    "\n",
    "    return avg_valid_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test model with aggregation as taking representation of last word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdzak\\AppData\\Local\\Temp\\ipykernel_22820\\1319915531.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix).to(device))\n"
     ]
    }
   ],
   "source": [
    "lstm_classifier_last = LSTMTextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim,num_layer,False, device, aggregate='last')\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(lstm_classifier_last.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdzak\\AppData\\Local\\Temp\\ipykernel_22820\\2511409481.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  text, labels, text_lengths  = torch.tensor(text).to(device), labels.to(device), text_lengths\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 1.4756, Validation Loss: 1.2941, Accuracy: 59.60%\n",
      "Epoch 2/50, Train Loss: 1.4863, Validation Loss: 1.6620, Accuracy: 22.40%\n",
      "Epoch 3/50, Train Loss: 1.3804, Validation Loss: 1.4556, Accuracy: 44.00%\n",
      "Epoch 4/50, Train Loss: 1.3101, Validation Loss: 1.2174, Accuracy: 67.80%\n",
      "Epoch 5/50, Train Loss: 1.2108, Validation Loss: 1.1993, Accuracy: 71.20%\n",
      "Epoch 6/50, Train Loss: 1.3215, Validation Loss: 1.2460, Accuracy: 66.40%\n",
      "Epoch 7/50, Train Loss: 1.3590, Validation Loss: 1.2532, Accuracy: 66.00%\n",
      "Epoch 8/50, Train Loss: 1.4442, Validation Loss: 1.4604, Accuracy: 44.40%\n",
      "Epoch 9/50, Train Loss: 1.4614, Validation Loss: 1.4291, Accuracy: 45.80%\n",
      "Epoch 10/50, Train Loss: 1.2585, Validation Loss: 1.1811, Accuracy: 73.00%\n",
      "Epoch 11/50, Train Loss: 1.1709, Validation Loss: 1.1828, Accuracy: 71.60%\n",
      "Epoch 12/50, Train Loss: 1.1253, Validation Loss: 1.1152, Accuracy: 79.20%\n",
      "Epoch 13/50, Train Loss: 1.1038, Validation Loss: 1.1284, Accuracy: 77.60%\n",
      "Epoch 14/50, Train Loss: 1.1112, Validation Loss: 1.1070, Accuracy: 80.00%\n",
      "Epoch 15/50, Train Loss: 1.0796, Validation Loss: 1.1013, Accuracy: 80.40%\n",
      "Epoch 16/50, Train Loss: 1.0755, Validation Loss: 1.0934, Accuracy: 81.20%\n",
      "Epoch 17/50, Train Loss: 1.0717, Validation Loss: 1.1130, Accuracy: 78.40%\n",
      "Epoch 18/50, Train Loss: 1.0694, Validation Loss: 1.0892, Accuracy: 81.60%\n",
      "Epoch 19/50, Train Loss: 1.0804, Validation Loss: 1.0802, Accuracy: 82.60%\n",
      "Epoch 20/50, Train Loss: 1.0547, Validation Loss: 1.0686, Accuracy: 83.80%\n",
      "Epoch 21/50, Train Loss: 1.0432, Validation Loss: 1.0768, Accuracy: 82.40%\n",
      "Epoch 22/50, Train Loss: 1.0451, Validation Loss: 1.0740, Accuracy: 83.20%\n",
      "Epoch 23/50, Train Loss: 1.0377, Validation Loss: 1.0638, Accuracy: 84.20%\n",
      "Epoch 24/50, Train Loss: 1.0260, Validation Loss: 1.0675, Accuracy: 83.60%\n",
      "Epoch 25/50, Train Loss: 1.0268, Validation Loss: 1.0551, Accuracy: 85.00%\n",
      "Epoch 26/50, Train Loss: 1.0354, Validation Loss: 1.0683, Accuracy: 83.60%\n",
      "Epoch 27/50, Train Loss: 1.0241, Validation Loss: 1.0889, Accuracy: 82.20%\n",
      "Epoch 28/50, Train Loss: 1.0516, Validation Loss: 1.0626, Accuracy: 84.20%\n",
      "Epoch 29/50, Train Loss: 1.0240, Validation Loss: 1.0749, Accuracy: 82.60%\n",
      "Epoch 30/50, Train Loss: 1.0201, Validation Loss: 1.0589, Accuracy: 84.40%\n",
      "Epoch 31/50, Train Loss: 1.0103, Validation Loss: 1.0626, Accuracy: 84.00%\n",
      "Epoch 32/50, Train Loss: 1.0097, Validation Loss: 1.0574, Accuracy: 84.80%\n",
      "Epoch 33/50, Train Loss: 1.0046, Validation Loss: 1.0703, Accuracy: 83.20%\n",
      "Epoch 34/50, Train Loss: 1.0116, Validation Loss: 1.0547, Accuracy: 84.40%\n",
      "Epoch 35/50, Train Loss: 0.9977, Validation Loss: 1.0510, Accuracy: 85.00%\n",
      "Epoch 36/50, Train Loss: 0.9984, Validation Loss: 1.0590, Accuracy: 84.40%\n",
      "Epoch 37/50, Train Loss: 1.0188, Validation Loss: 1.0493, Accuracy: 85.60%\n",
      "Epoch 38/50, Train Loss: 0.9938, Validation Loss: 1.0421, Accuracy: 85.60%\n",
      "Epoch 39/50, Train Loss: 0.9939, Validation Loss: 1.0494, Accuracy: 85.40%\n",
      "Epoch 40/50, Train Loss: 0.9902, Validation Loss: 1.0523, Accuracy: 84.40%\n",
      "Epoch 41/50, Train Loss: 0.9951, Validation Loss: 1.0522, Accuracy: 85.20%\n",
      "Epoch 42/50, Train Loss: 0.9939, Validation Loss: 1.0542, Accuracy: 84.80%\n",
      "Epoch 43/50, Train Loss: 0.9884, Validation Loss: 1.0391, Accuracy: 86.20%\n",
      "Epoch 44/50, Train Loss: 0.9834, Validation Loss: 1.0413, Accuracy: 86.20%\n",
      "Epoch 45/50, Train Loss: 0.9782, Validation Loss: 1.0398, Accuracy: 86.20%\n",
      "Epoch 46/50, Train Loss: 0.9818, Validation Loss: 1.0434, Accuracy: 85.80%\n",
      "Epoch 47/50, Train Loss: 0.9814, Validation Loss: 1.0429, Accuracy: 85.80%\n",
      "Epoch 48/50, Train Loss: 0.9768, Validation Loss: 1.0439, Accuracy: 85.80%\n",
      "Epoch 49/50, Train Loss: 0.9755, Validation Loss: 1.0456, Accuracy: 85.60%\n",
      "Epoch 50/50, Train Loss: 0.9801, Validation Loss: 1.0397, Accuracy: 86.20%\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "train(lstm_classifier_last, train_dataloader, development_dataloader, optimizer, loss_fn, device, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get test accuracy and loss for test datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 1.025850772857666\n",
      "Accuracy: 88.0\n"
     ]
    }
   ],
   "source": [
    "avg_valid_loss, accuracy = test(lstm_classifier_last, test_dataloader, device, loss_fn)\n",
    "\n",
    "print(\"Avg test loss:\", avg_valid_loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test model with aggregation as max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdzak\\AppData\\Local\\Temp\\ipykernel_22820\\1319915531.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix).to(device))\n"
     ]
    }
   ],
   "source": [
    "lstm_classifier_max = LSTMTextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim,num_layer,False, device, aggregate='max')\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(lstm_classifier_max.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdzak\\AppData\\Local\\Temp\\ipykernel_22820\\2511409481.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  text, labels, text_lengths  = torch.tensor(text).to(device), labels.to(device), text_lengths\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 1.3659, Validation Loss: 1.2386, Accuracy: 65.80%\n",
      "Epoch 2/50, Train Loss: 1.1730, Validation Loss: 1.1676, Accuracy: 73.00%\n",
      "Epoch 3/50, Train Loss: 1.1332, Validation Loss: 1.1111, Accuracy: 79.40%\n",
      "Epoch 4/50, Train Loss: 1.0952, Validation Loss: 1.0951, Accuracy: 81.40%\n",
      "Epoch 5/50, Train Loss: 1.1001, Validation Loss: 1.0866, Accuracy: 81.60%\n",
      "Epoch 6/50, Train Loss: 1.0712, Validation Loss: 1.0851, Accuracy: 82.60%\n",
      "Epoch 7/50, Train Loss: 1.0763, Validation Loss: 1.0834, Accuracy: 82.20%\n",
      "Epoch 8/50, Train Loss: 1.0897, Validation Loss: 1.1452, Accuracy: 75.60%\n",
      "Epoch 9/50, Train Loss: 1.0760, Validation Loss: 1.0935, Accuracy: 81.20%\n",
      "Epoch 10/50, Train Loss: 1.0509, Validation Loss: 1.0617, Accuracy: 84.40%\n",
      "Epoch 11/50, Train Loss: 1.0396, Validation Loss: 1.0707, Accuracy: 83.20%\n",
      "Epoch 12/50, Train Loss: 1.0343, Validation Loss: 1.0601, Accuracy: 84.40%\n",
      "Epoch 13/50, Train Loss: 1.0282, Validation Loss: 1.0760, Accuracy: 82.20%\n",
      "Epoch 14/50, Train Loss: 1.0232, Validation Loss: 1.0526, Accuracy: 85.60%\n",
      "Epoch 15/50, Train Loss: 1.0102, Validation Loss: 1.0503, Accuracy: 85.40%\n",
      "Epoch 16/50, Train Loss: 1.0124, Validation Loss: 1.0517, Accuracy: 85.40%\n",
      "Epoch 17/50, Train Loss: 1.0153, Validation Loss: 1.0522, Accuracy: 85.00%\n",
      "Epoch 18/50, Train Loss: 1.0019, Validation Loss: 1.1071, Accuracy: 78.40%\n",
      "Epoch 19/50, Train Loss: 1.0884, Validation Loss: 1.0624, Accuracy: 83.80%\n",
      "Epoch 20/50, Train Loss: 1.0055, Validation Loss: 1.0541, Accuracy: 85.20%\n",
      "Epoch 21/50, Train Loss: 0.9906, Validation Loss: 1.0503, Accuracy: 85.40%\n",
      "Epoch 22/50, Train Loss: 0.9930, Validation Loss: 1.0544, Accuracy: 84.80%\n",
      "Epoch 23/50, Train Loss: 0.9887, Validation Loss: 1.0434, Accuracy: 86.20%\n",
      "Epoch 24/50, Train Loss: 0.9811, Validation Loss: 1.0510, Accuracy: 85.20%\n",
      "Epoch 25/50, Train Loss: 0.9761, Validation Loss: 1.0435, Accuracy: 85.80%\n",
      "Epoch 26/50, Train Loss: 0.9758, Validation Loss: 1.0506, Accuracy: 85.20%\n",
      "Epoch 27/50, Train Loss: 0.9805, Validation Loss: 1.0483, Accuracy: 85.40%\n",
      "Epoch 28/50, Train Loss: 0.9755, Validation Loss: 1.0440, Accuracy: 85.80%\n",
      "Epoch 29/50, Train Loss: 0.9772, Validation Loss: 1.0459, Accuracy: 85.20%\n",
      "Epoch 30/50, Train Loss: 0.9701, Validation Loss: 1.0444, Accuracy: 85.40%\n",
      "Epoch 31/50, Train Loss: 0.9698, Validation Loss: 1.0481, Accuracy: 85.60%\n",
      "Epoch 32/50, Train Loss: 0.9685, Validation Loss: 1.0354, Accuracy: 86.60%\n",
      "Epoch 33/50, Train Loss: 0.9654, Validation Loss: 1.0337, Accuracy: 87.00%\n",
      "Epoch 34/50, Train Loss: 0.9662, Validation Loss: 1.0332, Accuracy: 86.60%\n",
      "Epoch 35/50, Train Loss: 0.9646, Validation Loss: 1.0302, Accuracy: 87.20%\n",
      "Epoch 36/50, Train Loss: 0.9596, Validation Loss: 1.0330, Accuracy: 86.20%\n",
      "Epoch 37/50, Train Loss: 0.9602, Validation Loss: 1.0389, Accuracy: 86.40%\n",
      "Epoch 38/50, Train Loss: 0.9618, Validation Loss: 1.0262, Accuracy: 87.60%\n",
      "Epoch 39/50, Train Loss: 0.9583, Validation Loss: 1.0512, Accuracy: 85.00%\n",
      "Epoch 40/50, Train Loss: 0.9620, Validation Loss: 1.0313, Accuracy: 86.60%\n",
      "Epoch 41/50, Train Loss: 0.9559, Validation Loss: 1.0267, Accuracy: 87.20%\n",
      "Epoch 42/50, Train Loss: 0.9764, Validation Loss: 1.0359, Accuracy: 86.40%\n",
      "Epoch 43/50, Train Loss: 0.9585, Validation Loss: 1.0337, Accuracy: 86.80%\n",
      "Epoch 44/50, Train Loss: 0.9538, Validation Loss: 1.0214, Accuracy: 88.00%\n",
      "Epoch 45/50, Train Loss: 0.9492, Validation Loss: 1.0138, Accuracy: 89.40%\n",
      "Epoch 46/50, Train Loss: 0.9468, Validation Loss: 1.0122, Accuracy: 89.60%\n",
      "Epoch 47/50, Train Loss: 0.9461, Validation Loss: 1.0113, Accuracy: 89.40%\n",
      "Epoch 48/50, Train Loss: 0.9502, Validation Loss: 1.0195, Accuracy: 88.40%\n",
      "Epoch 49/50, Train Loss: 0.9471, Validation Loss: 1.0131, Accuracy: 89.60%\n",
      "Epoch 50/50, Train Loss: 0.9473, Validation Loss: 1.0289, Accuracy: 87.60%\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "train(lstm_classifier_max, train_dataloader, development_dataloader, optimizer, loss_fn, device, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 1.0048667378723621\n",
      "Accuracy: 89.8\n"
     ]
    }
   ],
   "source": [
    "avg_valid_loss, accuracy = test(lstm_classifier_max, test_dataloader, device, loss_fn)\n",
    "\n",
    "print(\"Avg test loss:\", avg_valid_loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test model with aggregation as average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdzak\\AppData\\Local\\Temp\\ipykernel_22820\\1319915531.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix).to(device))\n"
     ]
    }
   ],
   "source": [
    "lstm_classifier_avg = LSTMTextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim,num_layer,False, device, aggregate='average')\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(lstm_classifier_avg.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdzak\\AppData\\Local\\Temp\\ipykernel_22820\\2511409481.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  text, labels, text_lengths  = torch.tensor(text).to(device), labels.to(device), text_lengths\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 1.4364, Validation Loss: 1.3318, Accuracy: 55.60%\n",
      "Epoch 2/50, Train Loss: 1.2499, Validation Loss: 1.2231, Accuracy: 69.80%\n",
      "Epoch 3/50, Train Loss: 1.2092, Validation Loss: 1.2106, Accuracy: 69.80%\n",
      "Epoch 4/50, Train Loss: 1.1742, Validation Loss: 1.1932, Accuracy: 72.00%\n",
      "Epoch 5/50, Train Loss: 1.1945, Validation Loss: 1.1941, Accuracy: 71.80%\n",
      "Epoch 6/50, Train Loss: 1.1763, Validation Loss: 1.2049, Accuracy: 70.40%\n",
      "Epoch 7/50, Train Loss: 1.1402, Validation Loss: 1.1524, Accuracy: 76.00%\n",
      "Epoch 8/50, Train Loss: 1.1429, Validation Loss: 1.1500, Accuracy: 76.20%\n",
      "Epoch 9/50, Train Loss: 1.1614, Validation Loss: 1.2965, Accuracy: 60.60%\n",
      "Epoch 10/50, Train Loss: 1.2201, Validation Loss: 1.1670, Accuracy: 74.20%\n",
      "Epoch 11/50, Train Loss: 1.1546, Validation Loss: 1.1529, Accuracy: 74.80%\n",
      "Epoch 12/50, Train Loss: 1.1172, Validation Loss: 1.1074, Accuracy: 80.20%\n",
      "Epoch 13/50, Train Loss: 1.1255, Validation Loss: 1.1157, Accuracy: 79.80%\n",
      "Epoch 14/50, Train Loss: 1.0943, Validation Loss: 1.0995, Accuracy: 81.00%\n",
      "Epoch 15/50, Train Loss: 1.0805, Validation Loss: 1.0948, Accuracy: 81.40%\n",
      "Epoch 16/50, Train Loss: 1.0840, Validation Loss: 1.0870, Accuracy: 82.60%\n",
      "Epoch 17/50, Train Loss: 1.0643, Validation Loss: 1.2345, Accuracy: 67.60%\n",
      "Epoch 18/50, Train Loss: 1.1015, Validation Loss: 1.0882, Accuracy: 82.60%\n",
      "Epoch 19/50, Train Loss: 1.0520, Validation Loss: 1.0881, Accuracy: 82.40%\n",
      "Epoch 20/50, Train Loss: 1.0508, Validation Loss: 1.0963, Accuracy: 81.00%\n",
      "Epoch 21/50, Train Loss: 1.0608, Validation Loss: 1.0897, Accuracy: 82.00%\n",
      "Epoch 22/50, Train Loss: 1.0508, Validation Loss: 1.0865, Accuracy: 82.60%\n",
      "Epoch 23/50, Train Loss: 1.0444, Validation Loss: 1.0785, Accuracy: 83.40%\n",
      "Epoch 24/50, Train Loss: 1.0433, Validation Loss: 1.0857, Accuracy: 82.20%\n",
      "Epoch 25/50, Train Loss: 1.0446, Validation Loss: 1.0645, Accuracy: 84.20%\n",
      "Epoch 26/50, Train Loss: 1.0627, Validation Loss: 1.0791, Accuracy: 82.80%\n",
      "Epoch 27/50, Train Loss: 1.0504, Validation Loss: 1.0813, Accuracy: 82.60%\n",
      "Epoch 28/50, Train Loss: 1.0502, Validation Loss: 1.0982, Accuracy: 80.80%\n",
      "Epoch 29/50, Train Loss: 1.0781, Validation Loss: 1.0946, Accuracy: 81.40%\n",
      "Epoch 30/50, Train Loss: 1.0471, Validation Loss: 1.0774, Accuracy: 83.00%\n",
      "Epoch 31/50, Train Loss: 1.0311, Validation Loss: 1.0603, Accuracy: 84.60%\n",
      "Epoch 32/50, Train Loss: 1.0262, Validation Loss: 1.0604, Accuracy: 84.00%\n",
      "Epoch 33/50, Train Loss: 1.0270, Validation Loss: 1.0706, Accuracy: 83.40%\n",
      "Epoch 34/50, Train Loss: 1.0260, Validation Loss: 1.0889, Accuracy: 81.20%\n",
      "Epoch 35/50, Train Loss: 1.0270, Validation Loss: 1.0643, Accuracy: 84.20%\n",
      "Epoch 36/50, Train Loss: 1.0191, Validation Loss: 1.0619, Accuracy: 84.60%\n",
      "Epoch 37/50, Train Loss: 1.0168, Validation Loss: 1.0642, Accuracy: 84.00%\n",
      "Epoch 38/50, Train Loss: 1.0138, Validation Loss: 1.0622, Accuracy: 84.00%\n",
      "Epoch 39/50, Train Loss: 1.0143, Validation Loss: 1.0629, Accuracy: 84.60%\n",
      "Epoch 40/50, Train Loss: 1.0332, Validation Loss: 1.0793, Accuracy: 82.80%\n",
      "Epoch 41/50, Train Loss: 1.0292, Validation Loss: 1.0752, Accuracy: 83.40%\n",
      "Epoch 42/50, Train Loss: 1.0103, Validation Loss: 1.0533, Accuracy: 85.00%\n",
      "Epoch 43/50, Train Loss: 1.0269, Validation Loss: 1.0599, Accuracy: 84.20%\n",
      "Epoch 44/50, Train Loss: 1.0141, Validation Loss: 1.0573, Accuracy: 85.00%\n",
      "Epoch 45/50, Train Loss: 1.0094, Validation Loss: 1.0611, Accuracy: 84.60%\n",
      "Epoch 46/50, Train Loss: 1.0085, Validation Loss: 1.0662, Accuracy: 84.20%\n",
      "Epoch 47/50, Train Loss: 1.0056, Validation Loss: 1.0696, Accuracy: 84.20%\n",
      "Epoch 48/50, Train Loss: 1.0079, Validation Loss: 1.0531, Accuracy: 85.40%\n",
      "Epoch 49/50, Train Loss: 1.0071, Validation Loss: 1.0578, Accuracy: 84.80%\n",
      "Epoch 50/50, Train Loss: 1.0030, Validation Loss: 1.0611, Accuracy: 84.40%\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "train(lstm_classifier_avg, train_dataloader, development_dataloader, optimizer, loss_fn, device, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 1.04337877035141\n",
      "Accuracy: 86.2\n"
     ]
    }
   ],
   "source": [
    "avg_valid_loss, accuracy = test(lstm_classifier_avg, test_dataloader, device, loss_fn)\n",
    "\n",
    "print(\"Avg test loss:\", avg_valid_loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
